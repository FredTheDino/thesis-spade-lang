\documentclass[msc,lith,english]{liuthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Imports
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,sorting=none,hyperref]{biblatex}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{tikz}
\usetikzlibrary{topaths,calc,tikzmark}
\usepackage{algorithm2e}
\usepackage{pgfgantt}
\usepackage{marginnote}
\usepackage{marvosym}
\usepackage[marginpar]{todo}
\usepackage{pgfgantt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\department{Institutionen för systemteknik}
\departmentenglish{Department of Electrical Engineering}
\departmentshort{ISY}

\supervisor{Frans Skarman}
\examiner{Oscar Gustavson}
\titleenglish{Seven implementations wordlength inference and one way that actually works}
\subtitleenglish{Wordlength inference in Spade-lang}
\titleswedish{Seven olika implementationer av ordlängdsinferans och en implementation som faktiskt fungerar}
\subtitleswedish{Ordlängdsinferans i Spade-lang}
\thesissubject{Datateknik}

\publicationyear{2023}
\currentyearthesisnumber{001}
\dateofpublication{2023-05-20}

\addbibresource{thesis.bib}

\renewcommand{\todomark}{TODO}

\author{Edvard Thörnros}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chaIntro}
\Todo{Is wordlength one word, two or maybe there should be a dash?}
\Todo{Just wrote something, dislike it}
Computers are a part of our everyday lives and all computers have something that computes, a compute unit. In some cases this is a dedicated CPU but in all cases it can is purpose built hardware. A modern way of building hardware is -- funnily enough -- by writing programs that can be mapped to hardware. If you want to create a special circuit you would not use a general purpose programming language (although subsets of Haskell can compile to hardware \cite{src:ClashExists}), these languages are referred to as Hardware Description Language, or HDL for short. One of these HDL is Spade, a HDL with a focus on usability which borrows much from the modern programming language \cite{src:spadeSomething} \cite{src:spadeAnHDL}. By writing high level code when describing hardware, we also open the door for optimizations and help from the compiler. This is the topic of this master thesis, a specific kind of optimization and user-help that can make Spade faster and safer.

This thesis focus on a specific kind of optimization, wordlength inference. Using a novel approach of combining wordlength inference with type inference. Wordlength is the number of bits to allocate to a value and when creating hardware you often have to specify this yourself. Getting this wordlength right everywhere can be tedious and time consuming. A small error in the wordlength might cause faults in the program and a too large wordlength wastes resources. %% \cite{src:BugsWithWordLength}. %% Some sources would be lovely here
Since the compiler has access to all the source code for the hardware, the compiler should be able to check the wordlengths everywhere and potentially optimize the wordlength where bits go unused. Putting these optimizations in the compiler allows code to be more general and easier to reuse, a common good practice in the software industry today. 

\section{Motivation}
Creating more powerful tools allows us to do more powerful things with them. In the case of software this effect is even larger, anyone with a laptop and a dream can develop programs for anyone to use. One of these fundamental tools is the compiler and programming languages -- no one in their right mind would use FORTRAN today when they have alternatives like Go, Rust or Python. Bringing this mindset to the hardware world could increase the productivity, usability and accessibility to custom circuits and accelerators. HDLs have the huge potential of improving all computation speeds in the world. It is questionably if this thesis alone will take us as far as to revolutionize the hardware industry, but it is certainly a step in the right direction for energy-efficient and faster computations.

\section{Research questions}
\begin{enumerate}
  \item How can interval arithmetic and affine arithmetic be used to implement wordlength inference?
  \item How does wordlength inference and optimization affect the number of LUTs, DPS-blocks and memories for a circuit?
  \item Can wordlength inference be used to create more reusable code?
\end{enumerate}

\section{Aim}
This thesis will implement wordlength inference in the Spade compiler using a combination of interval arithmetic and affine arithmetic. The implementation should then be evaluated using a synthesis tool and compared to other Spade-programs without these optimizations.

\section{Delimitations}
The sample size of the programs is quite limited, there is no attempt made to generalize the findings to all hardware. This thesis is limited to Spade and FPGAs and will not consider optimizations on other kinds of hardware. Other more sophisticated error-estimation like ME-gPC and modified addine arithmetic will not be studied.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chaBackground}
A sufficient basis to understand this work is presented in this chapter.

% Programming languages in general is considered one of the more mature topics in computer science. There are many, many books and papers written on compilers. Some programmers even consider compilers to be black boxes which they must obey, frightened of the complexities and details of the black magic working inside. This background aims to unbox these beasts called compilers and shed some light on them. Explaining all the details of a compiler and the type checker is out of scope for this background.
% 
% Wordlengths, HDLs and also get a section describing how they fit into this work.


\section{Introduction to Compiler Structure}
What constitutes a compiler is not always obvious. A compiler, in the most banal sense, takes an input program and outputs an output program. Some want the output to be ``simpler`` than the input, passing in a high level program in C and outputting executable X86 machine code where X86 is considered a ``simpler`` than C. The input to the compiler is often text, and we will assume this for the rest of this short introduction to compilers.

Each compiler is unique, but they often have a shared structure. The first step is often to do lexical analysis (also called lexing) in a lexer or tokenizer. Here characters are abstracted away, and the compiler has done the first processing of the text. When lexing you often decide what piece of text is an integer-constant, a keyword, a string, etc. After the lexing the tokens are used to perform semantic analysis -- parsing. During parsing the compiler understands structures in the program such as what is part of each function or correctly parsing the order of operations for mathematical expressions. The parsing usually produces an abstract syntax tree (AST). Though some compilers interweave these steps, they are usually there in spirit.

The Spade compiler has both a lexer step and a parser step which are located in different modules.

The compilers work is not done yet. After all the syntactical analysis the semantic analysis can be started, semantic analysis is sometimes referred to as the ``inner layers of the compiler``. Here we resolve identifiers, run type-checking and other static program analysis or do optimizations like moving around constants to avoid needless copies. The wordlength inference and optimizations will be an inner layer, the relevant details will be discussed in the Section \ref{sec:TypeChecking}. 

After the compiler has finished optimizing, the output is generated and potentially lowered (made less complex) in multiple stages, often by translating to a simpler internal representation which in the end makes generating the final output of the compiler easier.

One ``layer`` is often called a ``pass``. Spade is not a multi-pass compiler and perform these steps sequentially -- not interleaved.
\cite{src:DragonBook}\cite{src:CraftingInterp}\cite{src:KKLectures}

\begin{figure}
\begin{center}
\begin{tikzpicture}[xscale=3, yscale=2]
% vertical lines
\draw (0,0) -- (0,1.2) node[above, rotate=90] {Lexing};
\draw (0.4,0) -- (0.4,1.3) node[above, rotate=90] {Parsing};
\draw (0.8,0) -- (0.8,1.5) node[above, rotate=90] {Typechecking};
\draw (1.2,0) -- (1.2,1.3) node[above, rotate=90] {Optimizations};
\draw (1.6,0) -- (1.6,1.2) node[above, rotate=90] {Code Generation};
% horizontal line
\filldraw[color=green, fill=green] (0,0) -- (0.8,1.0) -- (1.6,0);
\draw (0.8,0.0) node[below] {``Available Information``};
\end{tikzpicture}
  \caption{A visualization of the rough measurement of information present in each step of the compilation process. Typechecking having the most information and lexing and code generation having the least amount of information.}
  \label{fig:InformationCompilation}
\end{center}
\end{figure}

Compilers have to construct a lot of complex information about the program. A visual some find helpful when reasoning about compilers is an imagined graph of ``available information``. Figure \ref{fig:InformationCompilation} tries to communicate the amount of information created in each step of compilation. The most important part being that we know a lot about the program in the type checking phase.

\todo{monomorphisation}

\section{Type Checking} % Maybe `inner layers`?
\label{sec:TypeChecking}
Type checking is a way of making sure the program is internally consistent, there are no contradictions inside to program to the program itself. Type checking can be done in different ways with different pros, cons or preferences \cite{src:TypeCheckersBook}. The type checker in Spade can infer types and deduce things about the program, like ``the first argument is a 3-bit integer value, but you gave a record`` \cite{src:spadeAnHDL}. Typechecking has shown to reduce some kinds of programming errors and can in some cases even suggest what functions to use.

The type checker in Spade is a one directional Damas–Hindley–Milner type checker. This means it stops on the first error and can deduce types to their most general form. So if asked to type check the identity function (a function that takes one value and returns the value as is, the function does nothing) the type checker would be able to deduce that the argument could have any type, but that the type is the same as the return value, without any help from the programmer except the body of the function. \cite{src:DamasHindleyMilner}

There is also a connected topic of type inference -- a program that guesses the types of expressions based on the context. A sufficiently good typeinferrer could be used to check the types of the program and can easily be modded into a typechecker. It is infact upon this idea that the Damas–Hindley–Milner typechecker works. In this paper, we will freely refer to typecheckers and work by typeinferrence as both typecheckers and typeinferrence -- even though it might not be the most correct according to the literature we deem it adds variety and clarity to how these systemms function.

\subsection{Unification}
\todo{Fill this in!}

\section{Interval Arithmetic, Affine Arithmetic and Self Validating Numerical Methods}
\label{sec:IAandAA}

There is an excellent explanation of both Interval- and Affine Arithmetic by \citeauthor{src:affAri} -- the following section is a short version to cover the absolute basics of the material.

Affine arithmetic and interval arithmetic and two common ways to estimate. They can be applied to estimate bounds for mathematical functions or things that can be modeled by mathematical functions. They also have a place in static analysis of programs, which is the focuses of this thesis. This section describes what these concepts are, references literature if you want to read more and shows their roll in static analysis of programs. These methods are often referred to as over-estimation.

\todo{Motivate why we chose to only use AA and IA}
Though affine arithmetic is more sophisticated it does not always produce better results, interval arithmetic can for some computations produce tighter bounds. There are other methods for overestimation that are considered more sophisticated like ME-gPC and modified affine arithmetic, but the extra complexity can be added later if it is found to be needed \cite{src:MEgPC}. For motivations of why are not considered see section \ref{sec:MotivationIAandAA}.

\subsection{Interval Arithmetic}
Interval arithmetic operates on intervals, as the name implies. A value -- or in the context of a program, a variable -- has a smallest and largest value it can assume. Consider \verb`x = random_real()`, where \verb`random_real` generates a random value in the range $[0, 1]$. We can express this in interval arithmetic as $\bar{x} = [0, 1]$, intervals will be denoted with a bar on top to separate them from the variables. Note especially that the true value of $x$ lies in the interval $\bar{x}$. In this example we know $0 \leq x \leq 1$, also written as $x \in [0, 1]$. These intervals can be added, negated, and so on, to give you an estimate of an arbitrary expression.

For a more throughout guide on implementations \citetitle{src:affAri} by \citeauthor{src:affAri} is worth your time, this section will cover the basics that are needed to understand the concept and understand this thesis. A full description of the field is out of scope.

\subsubsection{Interval Arithmetic Special values}
When doing static program analysis, some extra values are often defined. For example, it is okay to have an interval where one value is infinite. This interval $[5, \infty]$ is perfectly valid.

The empty interval is also defined and written as $[]$. The empty interval usually denotes expressions or code that cannot be reached or evaluated. It might seem useless at a glance, but is required to do more sophisticated static analysis.
\Todo{IA, have shorter definitions written in ``affine-and-interval-arithmatic.md``, maybe just refer to the original sources since it is not really relevant except for the implementation, unless we get a specific result due to one of them.}

\subsubsection{Interval Arithmetic: An example and Limitations}
Examples are often helpful.
We will be using the expression $2x + z - z$ as an example where $x = [0, 1], z = [1, 3]$.

\begin{verbatim}
2 * [0, 1] + [1, 3] - [1, 3]
// Scaling rule and subtraction is negated addition 
[2 * 0, 2 * 1] + [1, 3] + (-1 * [1, 3])
// Calculate
[0, 2] + [1, 3] + [-3, -1]
// Sum it upp
[-2, 4]
\end{verbatim}

This gives us the conclusion that this expression will lie in the range $[-2, 4]$ for the given values of $x$ and $z$. This is true, but the estimate is larger than it necessarily needs to be. An observant reader would notice that subtracting the value $z$ from itself should result in $0$, which is a perfectly valid point. This is a limitation of the interval arithmetic. Interval arithmetic does not reason about the expressions that came before it and how they combine, and this limitation would exist if used to do static analysis of programs. This limitation leads us to affine arithmetic which understands the relations between variables we evaluate. Affine arithmatic can more accurately calculate expressions like $a - a$ since it understands that $a$ and $a$ is the same thing.


\subsection{Affine Arithmetic}
Affine arithmetic (AA) works similarly to interval arithmetic (IA), but has a memory of where values come from and can reason about their combinations at a higher level. That said, AA does not produce strictly better results than IA in all circumstances and a combination is currently thought to be the current state of the art. 

\subsubsection{How Affine Arithmetic works}
In affine arithmetic there is a concept of noise symbols ($e_i$ where $i$ is a natural number) and the numbers half width ($x_i$ where $i$ is a natural number). A linear combination of these noise symbols is a reasonable way to represent a "number" when reasoning about affine arithmetic, $\hat{x} = x_0 + x_1e_1 + x_2e_2 + \dots$. These terms can them be combined using similar rules to interval arithmetic.

Notice how the first term lacks a noise symbol, this expresses where the center of the uncertainty is. The different noise variables serve as the memory of this expression, consider subtracting $\hat{x} - \hat{x}$ with itself we get the expected result of 0 from that.

\subsection{Error Explosion for Affine- and Interval Arithmetic}
Both affine arithmetic and interval arithmetic compensate for unknown factors with a span of potential values. This method is fine for most computations that are small and simple, but this isn't always the case. Some calculations result in large spans of potential values -- since a smaller span gives more certainty and are easier to account for.

For the use case of circuits and FPGAs which are relevant for Spade this can be a large problem. This problem is further discussed in section \ref{sec:MotivationIAandAA}.

\Todo{Don't know how good this is}

\cite{src:affAri}

\section{Wordlength Inference and Typechecking in Spade}
\label{sec:TheProblem}
Most values in Spade take up bits or space in the run time environment. Wordlength inference is mostly concerned with numbers -- so consider positive numbers without a decimal. Consider a program with a counter that resets to 0 after counting to 3, we do not need 32 bits to represent it. The cost of storing a number with 32 bits compared to 2 bits could be large if it requires a larger FPGA, extra circuit components or a different power rating. Compared to software where memory is considered abundant -- this causes Spade as a HDL to be designed differently compared to a highlevel programming language.

Wordlength inference is the compiler understanding what wordlength -- the number of bits -- is needed to store a value. Inferring this value well causes good resource usage and requires less manual intervention. Doing it poorly or not at all either requires users to manually specify the wordlength of each value, or a hardware that is inefficient.

\begin{figure}[h]
\begin{center}
\begin{verbatim}
fn f(a: int<3>) -> int<4> {
  a + 1 + 1
}

entity main(clk: clock, rst: bool) -> int<4> {
  f(3)
}
\end{verbatim}
\end{center}
\label{fig:SimpleFaultSpade}
\caption{``simple\_fault.spade`` A simple spade program that does not compile, showing the current limitations of wordlength inference.}
\end{figure}

\begin{figure}[h]
\begin{center}
\begin{verbatim}
error: Type error
  ┌─ src/simple\_fault.spade:1:27
  │
1 │   fn f(a: int<3>) -> int<4> {
  │                      ------ int<4> type specified here
  │ ╭───────────────────────────^
2 │ │   a + 1 + 1
3 │ │ }
  │ ╰─^ Found type int<5>
  │
  = Expected: 4
  =       in: int<4>
  =      Got: 5
  =       in: int<5>

Error: aborting due to previous error
\end{verbatim}
\end{center}
\label{fig:SimpleFaultSpadeCompileOutput}
\caption{The output from the compiler when trying to compile the program ``simple\_fault.spade``}
\end{figure}

Consider the program in Figure \ref{fig:SimpleFaultSpade}. The function ``f`` adds 3 values together, two of which are known constants. We also know that $2^3 + 2 < 2^4$ -- we should be able to fit the result of the addition into the 4 bit word without loss of data. The compiler does not agree as seen in the compiler output in Figure \ref{fig:SimpleFaultSpadeCompileOutput} where it claims we need 5 bits to store this value. This problem might seem inconsequential since calculating constant expressions during compilation would fix this, as seen by the program in Figure \ref{fig:SimpleCorrectSpade} compiling without worries. Only fixing this problem for constants would be nice variables would cause the same problem to appear but in a more difficult form. The problem here is much deeper, and is a direct cause of the typechecking algorithm itself.

\begin{figure}[h]
\begin{center}
\begin{verbatim}
fn f(a: int<3>) -> int<4> {
  a + 2
}

entity main(clk: clock, rst: bool) -> int<4> {
  f(3)
}
Error: aborting due to previous error
\end{verbatim}
\end{center}
\label{fig:SimpleCorrectSpade}
\caption{``simple\_correct.spade`` A spade program that does compile, which is very similar to the program in Figure \ref{fig:SimpleFaultSpade}}
\end{figure}

\todo{Maybe change to using screen shoots? But I like the idea of it being text since it's usually easier to read... Hmm...}
\todo{Move this to the typechecking section?}
The Spade compiler implements a the Damas-Hindley–Milner type checker -- a fast and modern type checker for simple type systems. Damas-Hindley-Milner runs in almost linear time (if implemented correctly) but is not complete. The completeness property for type checkers means there are programs which are correct but that the type checker will not recognize as correct. The programs from Figures \ref{fig:SimpleFaultSpade} and \ref{fig:SimpleCorrectSpade} are an example of this. This observation is important since it means this is not a bug, but a limitation of the compiler itself. How this should be changed and to what is the central topic of this thesis.

\section{Spade}
Spade is a HDL taking a lot of inspiration from Rust to create a more modern HDL (Hardware Description Language). Spade has syntax which mimics that of Rust and tries to remove problems people have when using other HDLs like System Verilog. One of the biggest features of Spade is the static typechecking which allows programs to be verified before even being synthesised -- creating a faster iteration loop.
\cite{src:spadeSomething} \cite{src:spadeAnHDL}

% \section{FPGA}
% \Todo{Explain what an FPGA is and what makes it different from micro controllers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Work}
\Todo{Needs a lot more content here, and it is a very well studied subject}

\section{Minimization of Fractional Wordlength on Fixed-Point Conversion for High-Level Synthesis}
Minimization of Fractional Wordlength on Fixed-Point Conversion for High-Level Synthesis suggests a method for using as few bits in the fractional part of values as possible. Their approach resulted in decent optimizations and was much faster than doing the optimization by hand. The sample size of the programs is quite small and did not always show as promising results.

\cite{src:MinOfFrac}

\section{High-level synthesis and arithmetic optimizations}
The thesis ``High-level synthesis and arithmetic optimizations`` is an attempt to merge the fields of arithmetic with and HDL-compilers. Several approaches are evaluated that affect correctness, throughput and latency. It describes fixed-point and floating point arithmetic, FPGA hardware and FPGA optimizations in a very clear way.

Problems with the IEEE floating point numbers are mentioned, for one how the implicit rounding causes addition to be non-associative, and how C/C++ compilers used for HDL use CPU optimizations which are not always suitable for FPGA.

\citeauthor{src:HLSandOpt} is an alternative approach to Spade when it comes to generating hardware and it tries to generate better FPGA hardware by modifying a compilation layer for the current compilers. But C/C++ has a fundamental problem when being mapped to hardware, the languages were built for single-threaded sequential computations where FPGAs prefer to do computations in parallel.

\cite{src:HLSandOpt}



% # Planned literature supporting the thesis
% - Books and papers on IA and AA
%   - Self-Validated Numerical Methods and Applications
% - Static analysis books and literature, potentially digging into bounded model checking (BMC) and the likes
%   - Calculus of Computation, maybe more literature here
% - The course on program analysis available on LiU 
%   - TDDE34 and the presentations there
% - Previouse literature on Spade
%   - Spade: An HDL Inspired by Modern Software Languages and co
% - Books and papers on type checkers and compilers books and literature
%   - Some papers on Henk are interesting
%   - Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism
%   - Types and Programming Languages by Ben Pierce
% - There is a lot of FPGA literature on FPGA optimizaton, here are a few
%    - Constantinides, George A.
%      Word-length Optimization for Differentiable Nonlinear Systems
%    - N. Doi and T. Horiyama and M. Nakanishi and S. Kimura
%      Minimization of fractional wordlength on fixed-point conversion for high-level synthesis
%    - Have like 10 more of these...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Method}
The method chapter gives a high level overview of the method used in this paper. More detailed steps are delegated to the implementation Chapter \ref{cha:Implementation}. Chapter \ref{cha:Implementation} contains implementation details and analysis of the implementation.

The problem is clearly defined from the previous chapters, especially Section \ref{sec:TheProblem} which goes into detail with examples. Alternatives for this implementation are then presented to the project manager of the Spade compiler -- which also happens to be an author in this paper -- for evaluation and consideration, the motivation for picking these are discussed in Section \ref{sec:ImplementationAlt} and one of them are selected for implementation. Deciding what to implement is critical and is a part of the development process.

After a viable solution is picked, it is time to implement that solution. Since the project is open source special care should be taken to make the changes easily accessible and available after this paper is written, since it is very much possible. A list of the repositories and versions of each software are available in the Appendix \ref{app:VersionsAndGitHashes} and the \href{https://github.com/FredTheDino/thesis-spade-lang}{git-repository for this thesis}. Steps of the implementation and details relevant to the results are discussed in the implementation details section -- Section \ref{sec:ImplementationDetails}. A full evaluation for each of the potential implementations and details about these implementations that are deemed relevant and interesting are mentioned. For full details it is probably best to read the source code. 

% TODO: Fill in this X with a number when I have example programs.
The implementation is then evaluated based on X example programs in Chapter \ref{cha:ImplementationEval} -- these programs are available in the Appendix \ref{app:Programs} and the \href{https://github.com/FredTheDino/thesis-spade-lang}{git-repository for this thesis}. Here the synthesis tool -- yosys in this thesis -- generates statistics that are collected before and after the changes to the Spade compiler -- including if the program type checked before or not. The information collected for each program with each version of the Spade compiler are:
\begin{itemize}
    \setlength\itemsep{0.5em}
    \item Version of the compiler used
    \item Does it type check
    \item Number of wires
    \item Number of wire bits
    \item Number of public wires
    \item Number of public wires bits
    \item Number of memories
    \item Number of memory bits
    \item Number of processes
    \item Number of cells
\end{itemize}
This information is then presented in Chapter \ref{cha:Results} and the discussed in Chapter \ref{cha:Discussion} -- named ``Results`` and ``Discussion``.

\section{Finding Implementations and Creative Problem Solving}
Solving complex problems is a creative process. Programmings languages are required to be complex since the ideas expressed in them are themselves complex. Complex problems require complex tools. The complexity requires that the programming language
has a certain degree of quality -- else the programming language quickly becomes useless and frustrating to use. To find a solution of sufficient quality requires trial and error and a good dose of analysis. Understanding partial solutions to a complex problem is a natural way of finding a solution that is higher in quality and hopefully good enough for the programming language to be usable. The Figure \ref{figCreativeProcess} contains a visualization of how these steps integrate with each other.

\begin{center}
\begin{figure}[h!]
\centering
\begin{tikzpicture}[thick, node/.style = {draw}]
  \node[node] (a) at (0,0) {Analyze implementation};
  \node[node] (b) at (0,3) {Implement and find errors};
  \node[node] (c) at (5,1.5) {External input};

  \draw[->, align=left] (a) to[out=30,in=-30] node[right]{Ideas} (b);
  \draw[->, align=left] (b) to[out=-150,in=150] node[left]{Deeper understanding} (a);
  \draw[->] (c) to[out=-90,in=0] node[right]{Concrete ideas} (a);
  \draw[->] (c) to[out=90,in=0] node[right]{Inspiration} (b);
\end{tikzpicture}
\caption{The creative process of solving complex problems according to the author.}
\label{figCreativeProcess}
\end{figure}
\end{center}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Results
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Results}
% \label{cha:Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementations and Analysis}
\label{cha:Implementation}
\todo{I removed the Results chapter and keept this instead, since I think all of this is results right?}
The problem of doing wordlength inference is a complex problem. This section contains a series of different implementations that failed in various ways, only the last one implementation ''worked'' and is described in Section \ref{sec:Seven}.

Each section describes the thought process, implementation details of most important, new insights gained from this work and an evaluation of the final state of the implementation for each attempted solution. Explicit commit hashes are given to make it very clear what version is discussed.

Starting the implementation work we knew parts of what we wanted to achieve. We wanted a more sophisticated version of wordlength inference that handled mathematical expressions with less error. The first step was understanding what was currently happening with the wordlength inference and where the problems where placed. The goal of this implementation was to find out just how much we did not know about the problem of a more sophisticated wordlength inferrer.

At the start, the dream was to implement this wordlength inference into the typeinference directly. It was considered somewhat possible since Damas-Hindley-Milner typecheckers have been implemented with sub-typing that could be mapped onto this problem -- one such mapping is Elm with typeinference and extensible records.

In the discussions, some of the less interesting implementation details are deliberately left out. The withheld details aim to make the text clearer and focused.

\section{Context and the Goal of These Experiments}
Most of these implementations were experiments and were not thought to be the solutions most likely to work. The more encompassing and maybe easier solutions have always been adding a separate module for wordlength inference -- in a similar way to how the linear types are checked in Spade at the moment -- or adding a whole new typeinference and typechecker.

The separate wordlength inference module was however not a completely sound solution. Splitting the wordlength inference from the typecheckeer has both pros and cons, since the information from the wordlength inference has to end up in the inferred types from the typeinference the modules need some kind of communication and this communication is obviously easier if the modules are the same module. However the wordlength inference requires information about the expressions and variables used in them, which the typeinference currently throws away. These kinds of errors with the interaction between the typeinference were suspected to be difficult to fix, so a few minor experiments were made into potential easier solutions.

Replacing the typeinference in the current compiler with something more powerful could potentially be the best solution. The current typeinference was regarded by some contributors as magic and was hard to work with and reason about -- in some sense the module was considered technical-debt. This poses a lot of language design questions, questions this thesis hoped to avoid since it might cause features to not be merged and made useful in the mainline compiler. Considering for example that a more powerful typeinference might not be desirable in the language. Since most of the more powerful typechecking-systems are know to have horrid error messages -- the antithesis of Spades goal of friendly error messages -- the change should not be made to quickly out of respect for the project. It would also be a very large undertaking to retrofit the entire compiler to work with this new typeinference and be more work than is available in this thesis and was always considered out of scope. 

\section{The First Implementation -- Naive}
\label{sec:First}
The changes for the first bash at the problem are placed on the git-branch \verb+the-simplest-implementation+ and has git-hash \verb+74c966a0317aa738017d2edf15def4719fe8dc95+.

After throwing a quick glance at the code it became clear that at least lower and upper bounds -- though these changes would not solve the larger problem of inferring the wordlength using Affine Arithmetic only allowing usage of Interval Arithmetic, but adding Affine Arithmetic in the typeinference required a kind of replacement that was unclear how to add at the time. It was also considered reasonable to only support wordlengths that fit in 64 bits -- since the current FPGA design does not handle that kind of data.

After poking some more two different ways of expressing constraints and requirement were found \verb+ConstraintExpr+ and \verb+Requirement+. These very similarly named concepts are very similar. \verb+ConstraintExpr+ is used primarily for compile-time integers and has support for addition, negation, constants and logarithms. \verb+Requirement+ adds information that does not fit neatly into a Damas–Hindley–Milner typesystem, namely: fields a type is expected to have, methods a type is expected to have and if a type can hold a given integer literal. Both \verb+Requirement+s and \verb+ConstraintExpr+s are removed after they are satisfied for a type.

The type checking of binary expressions simply added $1$ to the wordlength for sum-expressions, and summed up the wordlengths for multiplication-expressions. The naive worstcase approach was changed to work using Interval Arithmetic. 

\verb+ConstraintExpr+s looked to be the best suited current construction in the compiler so we decided to hijack it into supporting ranges, to see where the compiler started complaining. \verb+ConstraintExpr+ was changed to support addition, negation, and multiplication while working on a high and a low value not only a single value. The typechecking of binary expressions was also changed to add different constraints.

This change broke a lot of things in the compiler in interesting ways. Array lengths no longer worked since they also used these kinds of \verb+ConstraintExpr+ -- which is very interesting to note. The types of some expressions also failed to infer which complicated the \verb+HIR-lowering+. The reason for the missing types is because there are choices to be made when typechecking some expressions, what wordlength should actually be used when there are multiple candidates. There was also an attempt at mitigating the missing types by introducing an extra solver stage for the \verb+ConstraintExpr+s -- this stage ran after a function was typechecked and tried to pick the smallest size of all constraints. This change was also too large to allow as much inter-op as possible with the mainline spade-compiler.

The implementation outlined here did not work -- it was also never expected to work. It gave very clear hints as to where to dig deeper, where the understanding was sub par and some of the hidden requirements for adding wordlength inference.

\section{The Second Implementation -- Another Naive Stab}
\label{sec:Second}
The second attempt was also very early in the process. Changes are available on the git-branch \verb+the-second-simplest-thing+ and has git-hash \verb+3d92c0e4b28b64104f700c3d299f62e7938cb016+.

Full with unmotivated optimism the \verb+ConstraintExpr+ was extended to support a \verb+BitsToRange+ -- which is the opposite of the \verb+BitsToRepresent+ expression which handles logarithms. The idea was to allow moving between wordlength and a max-value in the type-level expressions. This would hopefully leave the array code working, while allowing the expression code to infer wordlengths at least somewhat freely. This idea and implementation -- was like the implemenetation in Section \ref{sec:First} -- not supposed to solve the entire problem and is not a fit solution for the entire problem since it only solves the sub-problem of very naive interval based wordlength inference.

When reviewing the code for this report an error in the original code was also found -- the function \verb+check_expr_for_replacement+ should refer to \verb+BitsToRange+ on line $1036$ -- the changes presented in this section never compiled.

These new constraints did not suffice, since all of the constraints were equality (\verb+=+) constraints and for this idea to work less-than-or-equal constrains were required (\verb+<=+). Less-than-or-equal constraints might be required for wordlength inference, and it might be why it is fairly hard and unstable to insert these constraints into the current constraint/requirement systems. The problem of adding Affine Arithmetic is still unsolved and would not be resolved using a solution of this form so we need to represent the operations and their values in full. We want to encode the operations and additions into a structure. We also need to carry the information for the intervals of variables somewhere else, since the current typesystem really disagrees with encoding ranges into int-types -- a new construction is required. 

\section{The Third Implementation -- Considering Equations}
\label{sec:Third}
The third attempt focused on adding equations and added a seperate constraint language. These constraints are part of the typeinference.

A new set of constraints was added giving \verb+ConstraintExpr+ and \verb+Requirement+ a new friend called \verb+SizeExpression+. Aside from the obvious technical-debt added with a whopping 3 different kinds of constraints that express almost the same thing -- the solution was deemed too ''ugly''. The constraints needed for \verb+SizeExpression+ were required to be feed forward by the typeinference -- following a completely different approach to the other requirements which were implicitly added to lists. This caused a large amount of changes in the typeinference which was preferably to be avoided.

We also start to suspect that the wordlength inference might \textit{need} to be a seperate module, split up from the typeinference and run after all the types have been inferred. This code did however not live long, and might even be considered mere curiosities.

\section{The Forth Implementation -- A Desperate Attempt}
Almost out of desperation for getting something to work. Just to make sure this whole ordeal wasn't impossible -- these changes were made. These changes are also the most insightful in all of these experiments. The changes are available on the git-branch \verb+the-fourth-attempt-now-with-equivelence+ and has git-hash \verb+ba0fa1baab56a725c31f703204da3b7d5f44380a+.

This implementation sent extra information about the largest number possible to come from the computations with all expressions -- similar to the third implementation in Section \ref{sec:Third} but just better written. \verb+ConstraintExpr+ was also extended with the variant \verb+LargestNumber+ which gives out the largest number representable by the wordlength given in -- the opposite of \verb+BitsToRepresent+ and similar to the extra variant made in the second implementation in Section \ref{sec:Second}. The constraints in the typechecking of binary expressions were changed to either take the largest number from the sub-expression or just the largest number from the wordlength this snippet is presented in Figure \ref{fig:BinaryExpression}. The code creates variables from sub-expressions and creates a hidden type-variable \verb+result_max+, then constraints link these different variables together -- all the constraints are equality (\verb+=+) constraints.

\begin{figure}
\begin{verbatim}
// Let the typechecker infer the maximum ints 
let lhs_max = self.visit_expression(&lhs, ctx, generic_list)?;
let rhs_max = self.visit_expression(&rhs, ctx, generic_list)?;
match *op {
   BinaryOperator::Add => {
       // Create new typevariables
       let (lhs_t, lhs_size) = self.new_split_generic_int(&ctx.symtab);
       let (rhs_t, rhs_size) = self.new_split_generic_int(&ctx.symtab);
       let (result_t, result_size) = self.new_split_generic_int(&ctx.symtab);

       // Create a helper type variable that adds indirection for the unification later
       let result_max = self.new_generic();

       // Take our best guess of a max value
       let lhs_max = match lhs_max {
           Some(max) => ce_var(&max),
           None => ce_largest(ce_var(&lhs_size)),
       };
       let rhs_max = match rhs_max {
           Some(max) => ce_var(&max),
           None => ce_largest(ce_var(&rhs_size)),
       };

      // We know the maximum values add to give a new maximum
      self.add_constraint(
          result_max.clone(),
          lhs_max.clone() + rhs_max.clone(),
          expression.loc(),
      );
      // The maximums implies a size for each of the values
      self.add_constraint(
          result_size.clone(),
          bits_to_store(ce_var(&result_max)),
          expression.loc(),
      );
      self.add_constraint(
          lhs_size.clone(),
          bits_to_store(lhs_max.clone()),
          lhs.loc(),
      );
      self.add_constraint(
          rhs_size.clone(),
          bits_to_store(rhs_max.clone()),
          rhs.loc(),
      );
      // Link the type variables to the AST
      self.unify_expression_generic_error(expression, &result_t, &ctx.symtab)?;
      self.unify_expression_generic_error(&lhs, &lhs_t, &ctx.symtab)?;
      self.unify_expression_generic_error(&rhs, &rhs_t, &ctx.symtab)?;

      // Return our best guess to make the expressions around aware of us
      Ok(Some(result_max))
   }
   ...
}
\end{verbatim}
\caption{The code adding the constraints for addition in the Spade-compiler}
\label{fig:BinaryExpression}
\end{figure}

This approach worked, for some definition of working. This solution behaves poorly and causes some very frustrating error messages. Since the hidden type-variable stored in \verb+result_max+ can cause errors in typevariables that are not able to be referenced in the source code of the program -- this kind of code can cause hard to debug errors. The layer of indirection caused by \verb+result_max+ is required to allow unification of int's of various max number into the same size. The \verb+result_max+ variable is also not unified in the unification process which causes us to create a lot of variables with largely the same information without them being synchronized, this can cause inconsistencies with the typechecker in edgecases. There is also extra information that is sent outside the AST and is not recorded. All of this points strongly to a different approach where a separate module tries to infer the wordlength.

\section{Other Implementations -- Just Curiosities}
\label{sec:Other}
There were two more attempts at an implementation inside the typechecker, they are available on the git-branches \verb+the-sixth-attempt+ and \verb+the-seventh-attempt-almost-as-simple-as-attempt-one+. Both these experiments yielded little of interest and are only mentioned out of completeness.

\section{The Seventh Implementations -- a Seperate Module}
\label{sec:Seven}
After doing a lot of thinking and reasoning we decided to see how a separate module would look -- in theory this would give maximum flexibility in how the wordlength inference worked. Using a separate module for the wordlength logic also meant changing it in the future would be easier, if more experiments would be conducted. Since the typechecker would not be changed as much old programs could still be compiled and thus compared with and without the more advanced wordlength inferrence. These changes are available on the branch \verb+the-seventh-attempt-almost-as-simple-as-attempt-one+ with git-hash \verb+889afd61a59f04f60730964b8ae7a2703110dd99+, these changes were merged and is available in the PR hosted on \url{https://gitlab.com/spade-lang/spade/-/merge_requests/200}.

The idea for the implementation was to walk the entire AST after typechecking had run and monomorphisation was complete -- each entity was given to the wordlength-inference module called \verb+spade-wordlength-inference+. The algorithm from Figure \ref{fig:WLIAlgo} was then applied. Visit the entire AST, picking out the expressions that are typed to be \verb+int<_>+ and construct algebraic constraints on the form \verb+<var> = <expression>+, this is called an equation. Pick out all typevariables which are \verb+int<_>+ from the previous step, populating a mapping between type-variables and their wordlength. Then loop over all constraints from step 1 and try to evaluate the right-hand side. The evaluation method can be one of the three modes: ''Affine Arithmetic'', ''Interval Arithmetic'' and ''smallest of Affine Arithmetic and Interval Arithmetic''. If evaluation succeeded add the new variable to a new known set of variables and check there are no contradictions. Stop the loop if no more progress is made or we have looped as many times as there are equations. The result is then unified back into the AST for safekeeping. There are of course more details, and the Rust code for wordlenght inference is hopefully quite readable. If you are interested I would recommend reading the source code directly.

\begin{figure}
\begin{verbatim}
(Variables, Equations) = extract_variables_and_equations(AST, TypeChecker)
Known = extract_known_types(Variables, AST)

for _ in Equations:
  KnownAtStart = Known
  for (Var, Body) in Equations:
    MaybeValue = case InferMethod of
                  IA -> evaluate_using_ia(Known, Body)
                  AA -> evaluate_using_aa(Known, Body)
                  AAIA -> min(evaluate_using_ia(Known, Body), evaluate_using_aa(Known, Body))
    case MaybeValue of
      Just Value -> inject_and_check_for_contradictions(Value, Known)
      Nothing -> pass
  if KnownAtStart == Known:
    break

for Var in Variables:
  unify(TypeChecker, Var, Known[Var])
\end{verbatim}
\caption{The algorithm used in the seventh attempt to determine the wordlength for all expressions in a spade-program.}
\label{fig:WLIAlgo}
\end{figure}

% - Show metrics compared on some example programs
% - Describe what worked well, and what didn't work well (like that users can't describe ranges)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Evaluation of the Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation of the Implementation}
\label{cha:ImplementationEval}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{cha:Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\label{cha:Conclusion}

\section{Future Work}

\clearpage

\printbibliography

\appendix
\chapter{Versions and Hashes}
\label{app:VersionsAndGitHashes}
\section{Git Hashes for Development}
All code is available on \href{https://gitlab.com/FredTheDino/spade}{the thesis GitLab}
\begin{verbatim}
branchname: the-simplest-implementation
git-commit: 74c966a0317aa738017d2edf15def4719fe8dc95

branchname: the-second-simplest-thing
git-commit: 3d92c0e4b28b64104f700c3d299f62e7938cb016

branchname: the-third-simplest-with-equations
git-commit: 83d1da48f5010767b9a96aea0a5bca13b7415084

branchname: the-fourth-attempt-now-with-equivelence
git-commit: ba0fa1baab56a725c31f703204da3b7d5f44380a

branchname: the-fifth-attempt-now-without-returns
git-commit: d8e9acbca755dea0c2ee78a014c578064c07d47e

branchname: the-sixth-attempt
git-commit: 11b3d060bc34145fda4918d255b186cb4057f07f

branchname: the-seventh-attempt-almost-as-simple-as-attempt-one
git-commit: 889afd61a59f04f60730964b8ae7a2703110dd99
url: https://gitlab.com/spade-lang/spade/-/merge_requests/200
\end{verbatim}

\section{Versions of programs}

\chapter{Programs for Evaluation}
\label{app:Programs}

\section{Program A -- A very dumb FIR-filter}

\subsection{pnr result}
\begin{verbatim}
swim clean; SPADE_INFER_METHOD=IA swim pnr
[INFO] Place and route components:
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)

swim clean; SPADE_INFER_METHOD=AAIA swim pnr
[INFO] Place and route components:
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)

swim clean; SPADE_INFER_METHOD=AA swim pnr
[INFO] Place and route components:
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)
\end{verbatim}

\subsection{Source Code}
\todo{Not sure I want to keep this in the report -- I don't think it adds much value, and a simple link to the gitlab page might suffice.}
\begin{verbatim}
fn xx<#C>(x: int<C..C>) -> int<0..100> {
  sext(x)
}

fn fir(fs: (int<0..100>, int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>)
, xs: (int<0..100>, int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>)) -> int<-500000..1000000> {
  fs#0 * xs#0 + fs#1 * xs#1 + fs#2 * xs#2 + fs#3 * xs#3 + fs#4 * xs#4 + fs#5 * xs#5 + fs#6 * xs#6 + fs#7 * xs#7 + fs#8 * xs#8 + fs#9 * xs#9 + fs#10 * xs#10 + fs#11 * xs#11 + fs#12 * xs#12 + fs#13 * xs#13 + fs#14 * xs#14 + fs#15 * xs#15 + fs#16 * xs#16 + fs#17 * xs#17 + fs#18 * xs#18 + fs#19 * xs#19 + fs#20 * xs#20 + fs#21 * xs#21 + fs#22 * xs#22 + fs#23 * xs#23 + fs#24 * xs#24 + fs#25 * xs#25 + fs#26 * xs#26 + fs#27 * xs#27 + fs#28 * xs#28 + fs#29 * xs#29 + fs#30 * xs#30 + fs#31 * xs#31 + fs#32 * xs#32 + fs#33 * xs#33 + fs#34 * xs#34 + fs#35 * xs#35 + fs#36 * xs#36 + fs#37 * xs#37 + fs#38 * xs#38 + fs#39 * xs#39 + fs#40 * xs#40 + fs#41 * xs#41 + fs#42 * xs#42 + fs#43 * xs#43 + fs#44 * xs#44 + fs#45 * xs#45 + fs#46 * xs#46 + fs#47 * xs#47 + fs#48 * xs#48 + fs#49 * xs#49 + fs#50 * xs#50 + fs#51 * xs#51 + fs#52 * xs#52 + fs#53 * xs#53 + fs#54 * xs#54 + fs#55 * xs#55 + fs#56 * xs#56 + fs#57 * xs#57 + fs#58 * xs#58 + fs#59 * xs#59 + fs#60 * xs#60 + fs#61 * xs#61 + fs#62 * xs#62 + fs#63 * xs#63 + fs#64 * xs#64 + fs#65 * xs#65 + fs#66 * xs#66 + fs#67 * xs#67 + fs#68 * xs#68 + fs#69 * xs#69 + fs#70 * xs#70 + fs#71 * xs#71 + fs#72 * xs#72 + fs#73 * xs#73 + fs#74 * xs#74 + fs#75 * xs#75 + fs#76 * xs#76 + fs#77 * xs#77 + fs#78 * xs#78 + fs#79 * xs#79 + fs#80 * xs#80 + fs#81 * xs#81 + fs#82 * xs#82 + fs#83 * xs#83 + fs#84 * xs#84 + fs#85 * xs#85 + fs#86 * xs#86 + fs#87 * xs#87 + fs#88 * xs#88 + fs#89 * xs#89 + fs#90 * xs#90 + fs#91 * xs#91 + fs#92 * xs#92 + fs#93 * xs#93 + fs#94 * xs#94 + fs#95 * xs#95 + fs#96 * xs#96 + fs#97 * xs#97 + fs#98 * xs#98 + fs#99 * xs#99
}

entity main(clk: clock, rst: bool) -> int<-500000..1000000> {
  let p : int<-500000..1000000> = fir(
    (xx(0), xx(1), xx(2), xx(3), xx(4), xx(5), xx(6), xx(7), xx(8), xx(9), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(20), xx(21), xx(22), xx(23), xx(24), xx(25), xx(26), xx(27), xx(28), xx(29), xx(30), xx(31), xx(32), xx(33), xx(34), xx(35), xx(36), xx(37), xx(38), xx(39), xx(40), xx(41), xx(42), xx(43), xx(44), xx(45), xx(46), xx(47), xx(48), xx(49), xx(50), xx(51), xx(52), xx(53), xx(54), xx(55), xx(56), xx(57), xx(58), xx(59), xx(60), xx(61), xx(62), xx(63), xx(64), xx(65), xx(66), xx(67), xx(68), xx(69), xx(70), xx(71), xx(72), xx(73), xx(74), xx(75), xx(76), xx(77), xx(78), xx(79), xx(80), xx(81), xx(82), xx(83), xx(84), xx(85), xx(86), xx(87), xx(88), xx(89), xx(90), xx(91), xx(92), xx(93), xx(94), xx(95), xx(96), xx(97), xx(98), xx(99),),

    (xx(99), xx(98), xx(97), xx(96), xx(95), xx(94), xx(93), xx(92), xx(91), xx(90), xx(89), xx(88), xx(87), xx(86), xx(85), xx(84), xx(83), xx(82), xx(81), xx(80), xx(79), xx(78), xx(77), xx(76), xx(75), xx(74), xx(73), xx(72), xx(71), xx(70), xx(69), xx(68), xx(67), xx(66), xx(65), xx(64), xx(63), xx(62), xx(61), xx(60), xx(59), xx(58), xx(57), xx(56), xx(55), xx(54), xx(53), xx(52), xx(51), xx(50), xx(49), xx(48), xx(47), xx(46), xx(45), xx(44), xx(43), xx(42), xx(41), xx(40), xx(39), xx(38), xx(37), xx(36), xx(35), xx(34), xx(33), xx(32), xx(31), xx(30), xx(29), xx(28), xx(27), xx(26), xx(25), xx(24), xx(23), xx(22), xx(21), xx(20), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(9), xx(8), xx(7), xx(6), xx(5), xx(4), xx(3), xx(2), xx(1), xx(0))
  );

  fir(
  ( xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19), xx(10), xx(11), xx(12), xx(13), xx(14), xx(15), xx(16), xx(17), xx(18), xx(19),),
  ( xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(19), xx(18), xx(17), xx(16), xx(15), xx(14), xx(13), xx(12), xx(11), xx(10), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1), xx(1))
  )
}
\end{verbatim}

\section{Program B}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Project Plan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project Plan}
An outline of the project. This section is a lot more informal and is expected to be removed before the final thesis is done.

\section{Method idea}
The method sketch looks like this:
\begin{enumerate}
  \item Implement wordlength deduction in Spade in a seperate module
  \item Make the wordlength deduction toggleable
  \item Find ~8 programs that represent a vide usage of Spade features
  \item Simulate the programs and make sure they behave identically
  \item Run a synthesis tool on the 8 programs with and without the wordlength deduction in a synthesis-tool. Note down the LUT usage, the expected power draw. The data gathered is the control data.
  \item Compare the results.
  \item Profit.
\end{enumerate}

\subsection{How the implementation will go}
From discussions not present in this document (see the thesis github page for more info) a separate ``module`` or ``wordlength inference engine`` will be made. This ``engine`` will be feed requirements in the form of a different kind of type equations. The variables will then be solved through a simple bi-directional satisfaction algorithm. We can then export this information into the type-variables we find. This discussion will be moved into the thesis soon, keeping it here for now tough. 

Here is a small and vauge-list of the implementation steps expected to take before starting the work.

\begin{enumerate}
    \item Add another set of ``type``-equations to the spade compiler, this step can be tested in isolation
    \item Make the type checker generate these type variables that can then be feed into the engine
    \item Make the ``wordlength inference engine`` that is compatible with the current behavior of Spade
    \item Simplify the variables to a single wordlength that the codegen can understand
    \item Make the ``wordlength inference engine`` that uses AA and IA
\end{enumerate}

The nice thing about this is that we can swap out the implementations easily. The code is isolated and easy to change and update. This change will not affect the syntax at all, but the syntax can be changed later.

\section{Expected Results}
It is expected that some situations the wordlength could be deduced and optimized. Since the current implementation is a strict over estimation there is some optimization fruit ripe for picking. The compiler should still be correct after these changes. In simple applications the inference should result in better wordlengths and decreased resource usage. The greatest gain will probably be in code re-usability and expressions being less directional.

\newpage
\section{GANTT-chart}

\begin{center}
\begin{figure}[htp]
\begin{ganttchart}[
    vgrid,
    bar left shift=0.15,
    bar right shift=-0.15,
    bar/.append style={rounded corners=3pt},
    bar inline label node/.style={%
      anchor=south, font=\ganttvalueof{bar label font}%
    },%
    inline,
    milestone inline label node/.style={%
      anchor=north, font=\ganttvalueof{milestone label font}%
    },%
    expand chart=\textwidth
]{1}{20}
\gantttitle{Project Plan}{20} \\
\gantttitlelist{1,...,20}{1} \\

\ganttbar{Scope project}{1}{3} \\
\ganttmilestone{The plan for the thesis}{3} \\

\ganttgroup{Implementation}{4}{13} \\
\ganttbar{Plan implementation}{4}{4}
\ganttbar{Write code}{5}{11}
\ganttbar{Evaluation}{12}{13}

\ganttmilestone{Version of WL-inference that can be demoed}{8} \\
\ganttmilestone{Preliminary results}{11} \\

\\
\ganttgroup{Thesis}{1}{19} \\
\ganttbar{Background}{1}{10}
\ganttmilestone{Find opponent}{16} \\

\ganttmilestone{1st hand in}{17}
\ganttbar{Related works}{4}{10} \\
\ganttbar{Method}{3}{5}

\ganttbar{Results}{11}{13}
\ganttbar{Conclusion}{15}{16} \\

\ganttbar{Discussion}{14}{15}
  \ganttbar{Polish report}{17}{17} \\
\ganttbar{Presentation}{18}{20} \\

\ganttmilestone[milestone inline label node/.style={anchor=north east}]{Hold presentation and final hand in}{20} \\

\ganttgroup{Research}{1}{8} \\
\ganttbar{Read papers}{1}{8} \\

% \ganttlink{elem11}{elem12}
% \ganttlink{elem11}{elem4}
% \ganttlink{elem5}{elem12}
% \ganttlink{elem10}{elem12}
% \ganttlink{elem9}{elem12}
% \ganttlink{elem22}{elem9}
% \ganttlink{elem22}{elem10}
% \ganttlink{elem7}{elem8}
% \ganttlink{elem6}{elem7}
% \ganttlink{elem8}{elem9}
% \ganttlink{elem9}{elem10}
\end{ganttchart}
  \caption{The project plan for the thesis project.}
  \label{figGanttChart}
\end{figure}
\end{center}

\subsection{Scope project}
Understand what the project is and what the goal (and ant-goals) of the project are. What kind of implementation is needed? Are there simpler ways of reaching the same goal? These kinds of questions are answered and result in the thesis plan.

\subsection{Implementation}
The project will contain implementation. During the implementation section, program code is changed to make the changes needed to the codebase.

\subsection{Plan implementation}
Here we decide how to implement what needs to be implemented. This will result in some kind of specification or more detailed plan that can be used as a jumping off point.

\subsection{Write code}
Here code is written to add the needed features to the compiler.

\subsection{Evaluation}
Here we look for problems with the implementation and run evaluations on the compiler to measure the difference in resource usage. 

\subsection{Thesis}
This section is all about the written thesis which is produced for this project.

\subsection{Background}
The background is written here.

\subsection{Related works}
Related works section is written here.

\subsection{Method}
The method is written and planned here.

\subsection{Result}
The result is written here.

\subsection{Conclusion}
The conclusion is written here

\subsection{Discussion}
The conclusion is written here

\subsection{Polish report}
The report is cleaned up and read through -- clearing up what needs to be clearing up.

\subsection{Presentation}
The presentation is worked on in this section.

\subsection{Research}
This section is for understanding the field and researching

\subsection{Read papers}
Papers in the field are read to understand the field more deeply and figure out how and what needs to be done.

\section{Resources}
Need access to FPGA simulation and synthesis software.

\section{Risks}

\newcommand{\riskHeader}[2]{(\textbf{likelihood:} #1, \textbf{severity:} #2)}
\begin{enumerate}
  \item \riskHeader{medium}{medium} A compiler is a complex piece of software, might miss something important when integrating my changes and this might cause the implementation to take extra time. Can be mitigated this by adding extra slack time in the plan for development.

  \item \riskHeader{low}{high} There are limitations to Damas–Hindley–Milner typesystems and things that cannot be done with them. Maybe wordlength induction interacts poorly with the type checker and bug or other incorrect behavior might sneak in.

  \item \riskHeader{medium}{low} Software might be hard to gauge the progress of if you do not understand all the details involved. Might have trouble communicating my progress or how development is progressing. Mitigation can be doing small commits and bundles of work that can be integrated early to show some kind of progress.

  \item \riskHeader{medium}{high} Might work less than planned each week due to other things in my life, unfortunately cannot plan around then only for them. This is extra prevalent because of already doing half a thesis. This will most likely drain my motivation and might result in me not finishing the thesis and getting my degree. Mitigation can be having fun, but this problem cannot be avoided or solved.

\end{enumerate}

\todos

\end{document}
