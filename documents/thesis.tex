\documentclass[msc,lith,english]{liuthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Imports
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,sorting=none,hyperref]{biblatex}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{tikz}
\usetikzlibrary{topaths,calc,tikzmark}
\usepackage{algorithm2e}
\usepackage{pgfgantt}
\usepackage{marginnote}
\usepackage{marvosym}
\usepackage[marginpar]{todo}
\usepackage{pgfgantt}
\usepackage{textcomp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\department{Institutionen för systemteknik}
\departmentenglish{Department of Electrical Engineering}
\departmentshort{ISY}

\supervisor{Frans Skarman}
\examiner{Oscar Gustavson}
\titleenglish{Seven implementations of wordlength inference and one implementation that actually works}
\subtitleenglish{Wordlength inference in Spade-lang}
\titleswedish{Sju olika implementationer av ordlängdsinferans och en implementation som faktiskt fungerar}
\subtitleswedish{Ordlängdsinferans i Spade-lang}
\thesissubject{Datateknik}

\publicationyear{2023}
\currentyearthesisnumber{001}
\dateofpublication{2023-05-20}

\addbibresource{thesis.bib}

\renewcommand{\todomark}{TODO}

\author{Edvard Thörnros}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chaIntro}
\Todo{Is wordlength one word, two or maybe there should be a dash?}
\Todo{Just wrote something, dislike it}
Computers are a part of our everyday lives and all computers have something that computes, a compute unit. In some cases this is a dedicated CPU but in all cases it can is purpose built hardware. A modern way of building hardware is -- funnily enough -- by writing programs that can be mapped to hardware. If you want to create a special circuit you would not use a general purpose programming language (although subsets of Haskell can compile to hardware \cite{src:ClashExists}), these languages are referred to as Hardware Description Language, or HDL for short. One of these HDL is Spade, a HDL with a focus on usability which borrows much from the modern programming language \cite{src:spadeSomething} \cite{src:spadeAnHDL}. By writing high level code when describing hardware, we also open the door for optimizations and help from the compiler. This is the topic of this master thesis, a specific kind of optimization and user-help that can make Spade faster and safer.

This thesis focus on a specific kind of optimization, wordlength inference. Using a novel approach of combining wordlength inference with type inference. Wordlength is the number of bits to allocate to a value and when creating hardware you often have to specify this yourself. Getting this wordlength right everywhere can be tedious and time consuming. A small error in the wordlength might cause faults in the program and a too large wordlength wastes resources. %% \cite{src:BugsWithWordLength}. %% Some sources would be lovely here
Since the compiler has access to all the source code for the hardware, the compiler should be able to check the wordlengths everywhere and potentially optimize the wordlength where bits go unused. Putting these optimizations in the compiler allows code to be more general and easier to reuse, a common good practice in the software industry today. 

\section{Motivation}
Creating more powerful tools allows us to do more powerful things with them. In the case of software this effect is even larger, anyone with a laptop and a dream can develop programs for anyone to use. One of these fundamental tools is the compiler and programming languages -- no one in their right mind would use FORTRAN today when they have alternatives like Go, Rust or Python. Bringing this mindset to the hardware world could increase the productivity, usability and accessibility to custom circuits and accelerators. HDLs have the huge potential of improving all computation speeds in the world. It is questionably if this thesis alone will take us as far as to revolutionize the hardware industry, but it is certainly a step in the right direction for energy-efficient and faster computations.

\section{Research questions}
\begin{enumerate}
  \item How can interval arithmetic and affine arithmetic be used to implement wordlength inference?
  \item How does wordlength inference and optimization affect the number of LUTs, DPS-blocks and memories for a circuit?
  \item Can wordlength inference be used to create more reusable code?
\end{enumerate}

\section{Aim}
This thesis will implement wordlength inference in the Spade compiler using a combination of interval arithmetic and affine arithmetic. The implementation should then be evaluated using a synthesis tool and compared to other Spade-programs without these optimizations.

\section{Delimitations}
The sample size of the programs is quite limited, there is no attempt made to generalize the findings to all hardware. This thesis is limited to Spade and FPGAs and will not consider optimizations on other kinds of hardware. Other more sophisticated error-estimation like ME-gPC and modified addine arithmetic will not be studied.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chaBackground}
A sufficient basis to understand this work is presented in this chapter.

% Programming languages in general is considered one of the more mature topics in computer science. There are many, many books and papers written on compilers. Some programmers even consider compilers to be black boxes which they must obey, frightened of the complexities and details of the black magic working inside. This background aims to unbox these beasts called compilers and shed some light on them. Explaining all the details of a compiler and the type checker is out of scope for this background.
% 
% Wordlengths, HDLs and also get a section describing how they fit into this work.


\section{Introduction to Compiler Structure}
What constitutes a compiler is not always obvious. A compiler, in the most banal sense, takes an input program and outputs an output program. Some want the output to be ``simpler`` than the input, passing in a high level program in C and outputting executable X86 machine code where X86 is considered a ``simpler`` than C. The input to the compiler is often text, and we will assume this for the rest of this short introduction to compilers.

Each compiler is unique, but they often have a shared structure. The first step is often to do lexical analysis (also called lexing) in a lexer or tokenizer. Here characters are abstracted away, and the compiler has done the first processing of the text. When lexing you often decide what piece of text is an integer-constant, a keyword, a string, etc. After the lexing the tokens are used to perform semantic analysis -- parsing. During parsing the compiler understands structures in the program such as what is part of each function or correctly parsing the order of operations for mathematical expressions. The parsing usually produces an abstract syntax tree (AST). Though some compilers interweave these steps, they are usually there in spirit.

The Spade compiler has both a lexer step and a parser step which are located in different modules.

The compilers work is not done yet. After all the syntactical analysis the semantic analysis can be started, semantic analysis is sometimes referred to as the ``inner layers of the compiler``. Here we resolve identifiers, run type-checking and other static program analysis or do optimizations like moving around constants to avoid needless copies. The wordlength inference and optimizations will be an inner layer, the relevant details will be discussed in the Section \ref{sec:TypeChecking}. 

After the compiler has finished optimizing, the output is generated and potentially lowered (made less complex) in multiple stages, often by translating to a simpler internal representation which in the end makes generating the final output of the compiler easier.

One ``layer`` is often called a ``pass``. Spade is not a multi-pass compiler and perform these steps sequentially -- not interleaved.
\cite{src:DragonBook}\cite{src:CraftingInterp}\cite{src:KKLectures}

\begin{figure}
\begin{center}
\begin{tikzpicture}[xscale=3, yscale=2]
% vertical lines
\draw (0,0) -- (0,1.2) node[above, rotate=90] {Lexing};
\draw (0.4,0) -- (0.4,1.3) node[above, rotate=90] {Parsing};
\draw (0.8,0) -- (0.8,1.5) node[above, rotate=90] {Typechecking};
\draw (1.2,0) -- (1.2,1.3) node[above, rotate=90] {Optimizations};
\draw (1.6,0) -- (1.6,1.2) node[above, rotate=90] {Code Generation};
% horizontal line
\filldraw[color=green, fill=green] (0,0) -- (0.8,1.0) -- (1.6,0);
\draw (0.8,0.0) node[below] {``Available Information``};
\end{tikzpicture}
  \caption{A visualization of the rough measurement of information present in each step of the compilation process. Typechecking having the most information and lexing and code generation having the least amount of information.}
  \label{fig:InformationCompilation}
\end{center}
\end{figure}

Compilers have to construct a lot of complex information about the program. A visual some find helpful when reasoning about compilers is an imagined graph of ``available information``. Figure \ref{fig:InformationCompilation} tries to communicate the amount of information created in each step of compilation. The most important part being that we know a lot about the program in the type checking phase.

\todo{monomorphisation}

\section{Type Checking} % Maybe `inner layers`?
\label{sec:TypeChecking}
Type checking is a way of making sure the program is internally consistent, there are no contradictions inside to program to the program itself. Type checking can be done in different ways with different pros, cons or preferences \cite{src:TypeCheckersBook}. The type checker in Spade can infer types and deduce things about the program, like ``the first argument is a 3-bit integer value, but you gave a record`` \cite{src:spadeAnHDL}. Typechecking has shown to reduce some kinds of programming errors and can in some cases even suggest what functions to use.

The type checker in Spade is a one directional Damas–Hindley–Milner type checker. This means it stops on the first error and can deduce types to their most general form. So if asked to type check the identity function (a function that takes one value and returns the value as is, the function does nothing) the type checker would be able to deduce that the argument could have any type, but that the type is the same as the return value, without any help from the programmer except the body of the function. \cite{src:DamasHindleyMilner}

There is also a connected topic of type inference -- a program that guesses the types of expressions based on the context. A sufficiently good typeinferrer could be used to check the types of the program and can easily be modded into a typechecker. It is infact upon this idea that the Damas–Hindley–Milner typechecker works. In this paper, we will freely refer to typecheckers and work by typeinferrence as both typecheckers and typeinferrence -- even though it might not be the most correct according to the literature we deem it adds variety and clarity to how these systemms function.

\subsection{Unification}
\todo{Fill this in!}

\section{Interval Arithmetic, Affine Arithmetic and Self Validating Numerical Methods}
\label{sec:IAandAA}

There is an excellent explanation of both Interval- and Affine Arithmetic by \citeauthor{src:affAri} -- the following section is a short version to cover the absolute basics of the material.

Affine arithmetic and interval arithmetic and two common ways to estimate. They can be applied to estimate bounds for mathematical functions or things that can be modeled by mathematical functions. They also have a place in static analysis of programs, which is the focuses of this thesis. This section describes what these concepts are, references literature if you want to read more and shows their roll in static analysis of programs. These methods are often referred to as over-estimation.

\todo{Motivate why we chose to only use AA and IA}
Though affine arithmetic is more sophisticated it does not always produce better results, interval arithmetic can for some computations produce tighter bounds. There are other methods for overestimation that are considered more sophisticated like ME-gPC and modified affine arithmetic, but the extra complexity can be added later if it is found to be needed \cite{src:MEgPC}. For motivations of why are not considered see section \ref{sec:MotivationIAandAA}.

\subsection{Interval Arithmetic}
Interval arithmetic operates on intervals, as the name implies. A value -- or in the context of a program, a variable -- has a smallest and largest value it can assume. Consider \verb`x = random_real()`, where \verb`random_real` generates a random value in the range $[0, 1]$. We can express this in interval arithmetic as $\bar{x} = [0, 1]$, intervals will be denoted with a bar on top to separate them from the variables. Note especially that the true value of $x$ lies in the interval $\bar{x}$. In this example we know $0 \leq x \leq 1$, also written as $x \in [0, 1]$. These intervals can be added, negated, and so on, to give you an estimate of an arbitrary expression.

For a more throughout guide on implementations \citetitle{src:affAri} by \citeauthor{src:affAri} is worth your time, this section will cover the basics that are needed to understand the concept and understand this thesis. A full description of the field is out of scope.

\subsubsection{Interval Arithmetic Special values}
When doing static program analysis, some extra values are often defined. For example, it is okay to have an interval where one value is infinite. This interval $[5, \infty]$ is perfectly valid.

The empty interval is also defined and written as $[]$. The empty interval usually denotes expressions or code that cannot be reached or evaluated. It might seem useless at a glance, but is required to do more sophisticated static analysis.
\Todo{IA, have shorter definitions written in ``affine-and-interval-arithmatic.md``, maybe just refer to the original sources since it is not really relevant except for the implementation, unless we get a specific result due to one of them.}

\subsubsection{Interval Arithmetic: An example and Limitations}
Examples are often helpful.
We will be using the expression $2x + z - z$ as an example where $x = [0, 1], z = [1, 3]$.

\begin{verbatim}
2 * [0, 1] + [1, 3] - [1, 3]
// Scaling rule and subtraction is negated addition 
[2 * 0, 2 * 1] + [1, 3] + (-1 * [1, 3])
// Calculate
[0, 2] + [1, 3] + [-3, -1]
// Sum it upp
[-2, 4]
\end{verbatim}

This gives us the conclusion that this expression will lie in the range $[-2, 4]$ for the given values of $x$ and $z$. This is true, but the estimate is larger than it necessarily needs to be. An observant reader would notice that subtracting the value $z$ from itself should result in $0$, which is a perfectly valid point. This is a limitation of the interval arithmetic. Interval arithmetic does not reason about the expressions that came before it and how they combine, and this limitation would exist if used to do static analysis of programs. This limitation leads us to affine arithmetic which understands the relations between variables we evaluate. Affine arithmatic can more accurately calculate expressions like $a - a$ since it understands that $a$ and $a$ is the same thing.


\subsection{Affine Arithmetic}
Affine arithmetic (AA) works similarly to interval arithmetic (IA), but has a memory of where values come from and can reason about their combinations at a higher level. That said, AA does not produce strictly better results than IA in all circumstances and a combination is currently thought to be the current state of the art. 

\subsubsection{How Affine Arithmetic works}
In affine arithmetic there is a concept of noise symbols ($e_i$ where $i$ is a natural number) and the numbers half width ($x_i$ where $i$ is a natural number). A linear combination of these noise symbols is a reasonable way to represent a "number" when reasoning about affine arithmetic, $\hat{x} = x_0 + x_1e_1 + x_2e_2 + \dots$. These terms can them be combined using similar rules to interval arithmetic.

Notice how the first term lacks a noise symbol, this expresses where the center of the uncertainty is. The different noise variables serve as the memory of this expression, consider subtracting $\hat{x} - \hat{x}$ with itself we get the expected result of 0 from that.

\Todo{Something about how multiplication works}

\subsection{Error Explosion for Affine- and Interval Arithmetic}
Both affine arithmetic and interval arithmetic compensate for unknown factors with a span of potential values. This method is fine for most computations that are small and simple, but this isn't always the case. Some calculations result in large spans of potential values -- since a smaller span gives more certainty and are easier to account for.

For the use case of circuits and FPGAs which are relevant for Spade this can be a large problem. This problem is further discussed in section \ref{sec:MotivationIAandAA}.

\Todo{Don't know how good this is}

\cite{src:affAri}

\section{Wordlength Inference and Typechecking in Spade}
\label{sec:TheProblem}
Most values in Spade take up bits or space in the run time environment. Wordlength inference is mostly concerned with numbers -- so consider positive numbers without a decimal. Consider a program with a counter that resets to 0 after counting to 3, we do not need 32 bits to represent it. The cost of storing a number with 32 bits compared to 2 bits could be large if it requires a larger FPGA, extra circuit components or a different power rating. Compared to software where memory is considered abundant -- this causes Spade as a HDL to be designed differently compared to a highlevel programming language.

Wordlength inference is the compiler understanding what wordlength -- the number of bits -- is needed to store a value. Inferring this value well causes good resource usage and requires less manual intervention. Doing it poorly or not at all either requires users to manually specify the wordlength of each value, or a hardware that is inefficient.

\begin{figure}[h]
\begin{center}
\begin{verbatim}
fn f(a: int<3>) -> int<4> {
  a + 1 + 1
}

entity main(clk: clock, rst: bool) -> int<4> {
  f(3)
}
\end{verbatim}
\end{center}
\label{fig:SimpleFaultSpade}
\caption{``simple\_fault.spade`` A simple spade program that does not compile, showing the current limitations of wordlength inference.}
\end{figure}

\begin{figure}[h]
\begin{center}
\begin{verbatim}
error: Type error
  ┌─ src/simple\_fault.spade:1:27
  │
1 │   fn f(a: int<3>) -> int<4> {
  │                      ------ int<4> type specified here
  │ ╭───────────────────────────^
2 │ │   a + 1 + 1
3 │ │ }
  │ ╰─^ Found type int<5>
  │
  = Expected: 4
  =       in: int<4>
  =      Got: 5
  =       in: int<5>

Error: aborting due to previous error
\end{verbatim}
\end{center}
\label{fig:SimpleFaultSpadeCompileOutput}
\caption{The output from the compiler when trying to compile the program ``simple\_fault.spade``}
\end{figure}

Consider the program in Figure \ref{fig:SimpleFaultSpade}. The function ``f`` adds 3 values together, two of which are known constants. We also know that $2^3 + 2 < 2^4$ -- we should be able to fit the result of the addition into the 4 bit word without loss of data. The compiler does not agree as seen in the compiler output in Figure \ref{fig:SimpleFaultSpadeCompileOutput} where it claims we need 5 bits to store this value. This problem might seem inconsequential since calculating constant expressions during compilation would fix this, as seen by the program in Figure \ref{fig:SimpleCorrectSpade} compiling without worries. Only fixing this problem for constants would be nice variables would cause the same problem to appear but in a more difficult form. The problem here is much deeper, and is a direct cause of the typechecking algorithm itself.

\begin{figure}[h]
\begin{center}
\begin{verbatim}
fn f(a: int<3>) -> int<4> {
  a + 2
}

entity main(clk: clock, rst: bool) -> int<4> {
  f(3)
}
Error: aborting due to previous error
\end{verbatim}
\end{center}
\label{fig:SimpleCorrectSpade}
\caption{``simple\_correct.spade`` A spade program that does compile, which is very similar to the program in Figure \ref{fig:SimpleFaultSpade}}
\end{figure}

\todo{Maybe change to using screen shoots? But I like the idea of it being text since it's usually easier to read... Hmm...}
\todo{Move this to the typechecking section?}
The Spade compiler implements a the Damas-Hindley–Milner type checker -- a fast and modern type checker for simple type systems. Damas-Hindley-Milner runs in almost linear time (if implemented correctly) but is not complete. The completeness property for type checkers means there are programs which are correct but that the type checker will not recognize as correct. The programs from Figures \ref{fig:SimpleFaultSpade} and \ref{fig:SimpleCorrectSpade} are an example of this. This observation is important since it means this is not a bug, but a limitation of the compiler itself. How this should be changed and to what is the central topic of this thesis.

\section{Spade}
Spade is a HDL taking a lot of inspiration from Rust to create a more modern HDL (Hardware Description Language). Spade has syntax which mimics that of Rust and tries to remove problems people have when using other HDLs like System Verilog. One of the biggest features of Spade is the static typechecking which allows programs to be verified before even being synthesised -- creating a faster iteration loop.
\cite{src:spadeSomething} \cite{src:spadeAnHDL}

% \section{FPGA}
\Todo{Explain what an FPGA is and what makes it different from micro controllers}

\Todo{I need to explain Swim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Work}
\Todo{Needs a lot more content here, and it is a very well studied subject}

\section{Minimization of Fractional Wordlength on Fixed-Point Conversion for High-Level Synthesis}
Minimization of Fractional Wordlength on Fixed-Point Conversion for High-Level Synthesis suggests a method for using as few bits in the fractional part of values as possible. Their approach resulted in decent optimizations and was much faster than doing the optimization by hand. The sample size of the programs is quite small and did not always show as promising results.

\cite{src:MinOfFrac}

\section{High-level synthesis and arithmetic optimizations}
The thesis ``High-level synthesis and arithmetic optimizations`` is an attempt to merge the fields of arithmetic with and HDL-compilers. Several approaches are evaluated that affect correctness, throughput and latency. It describes fixed-point and floating point arithmetic, FPGA hardware and FPGA optimizations in a very clear way.

Problems with the IEEE floating point numbers are mentioned, for one how the implicit rounding causes addition to be non-associative, and how C/C++ compilers used for HDL use CPU optimizations which are not always suitable for FPGA.

\citeauthor{src:HLSandOpt} is an alternative approach to Spade when it comes to generating hardware and it tries to generate better FPGA hardware by modifying a compilation layer for the current compilers. But C/C++ has a fundamental problem when being mapped to hardware, the languages were built for single-threaded sequential computations where FPGAs prefer to do computations in parallel.

\cite{src:HLSandOpt}

\Todo{Software development methods}



% # Planned literature supporting the thesis
% - Books and papers on IA and AA
%   - Self-Validated Numerical Methods and Applications
% - Static analysis books and literature, potentially digging into bounded model checking (BMC) and the likes
%   - Calculus of Computation, maybe more literature here
% - The course on program analysis available on LiU 
%   - TDDE34 and the presentations there
% - Previouse literature on Spade
%   - Spade: An HDL Inspired by Modern Software Languages and co
% - Books and papers on type checkers and compilers books and literature
%   - Some papers on Henk are interesting
%   - Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism
%   - Types and Programming Languages by Ben Pierce
% - There is a lot of FPGA literature on FPGA optimizaton, here are a few
%    - Constantinides, George A.
%      Word-length Optimization for Differentiable Nonlinear Systems
%    - N. Doi and T. Horiyama and M. Nakanishi and S. Kimura
%      Minimization of fractional wordlength on fixed-point conversion for high-level synthesis
%    - Have like 10 more of these...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Method}
The method chapter gives a high level overview of the method used in this paper. More detailed steps are delegated to the implementation Chapter \ref{cha:Implementation}. Chapter \ref{cha:Implementation} contains implementation details and analysis of the implementation.

The problem is clearly defined from the previous chapters, especially Section \ref{sec:TheProblem} which goes into detail with examples. Alternatives for this implementation are then presented to the project manager of the Spade compiler -- which also happens to be an author in this paper -- for evaluation and consideration, the motivation for picking these are discussed in Section \ref{sec:ImplementationAlt} and one of them are selected for implementation. Deciding what to implement is critical and is a part of the development process.

After a viable solution is picked, it is time to implement that solution. Since the project is open source special care should be taken to make the changes easily accessible and available after this paper is written, since it is very much possible. A list of the repositories and versions of each software are available in the Appendix \ref{app:VersionsAndGitHashes} and the \href{https://github.com/FredTheDino/thesis-spade-lang}{git-repository for this thesis}. Steps of the implementation and details relevant to the results are discussed in the implementation details section -- Section \ref{sec:ImplementationDetails}. A full evaluation for each of the potential implementations and details about these implementations that are deemed relevant and interesting are mentioned. For full details it is probably best to read the source code. 

% TODO: Fill in this X with a number when I have example programs.
The implementation is then evaluated based on X example programs in Chapter \ref{cha:ImplementationEval} -- these programs are available in the Appendix \ref{app:Programs} and the \href{https://github.com/FredTheDino/thesis-spade-lang}{git-repository for this thesis}. Here the synthesis tool -- yosys in this thesis -- generates statistics that are collected before and after the changes to the Spade compiler -- including if the program type checked before or not. The information collected for each program with each version of the Spade compiler are:

\todo{Update this}
\begin{itemize}
    \setlength\itemsep{0.5em}
    \item Version of the compiler used
    \item Does it type check
    \item TODO: This needs updating!
    \item Number of wires
    \item Number of wire bits
    \item Number of public wires
    \item Number of public wires bits
    \item Number of memories
    \item Number of memory bits
    \item Number of processes
    \item Number of cells
\end{itemize}
This information is then presented in Chapter \ref{cha:Results} and the discussed in Chapter \ref{cha:Discussion} -- named ``Results`` and ``Discussion``.

\section{Finding Implementations and Creative Problem Solving}
Solving complex problems is a creative process. Programmings languages are required to be complex since the ideas expressed in them are themselves complex. Complex problems require complex tools. The complexity requires that the programming language
has a certain degree of quality -- else the programming language quickly becomes useless and frustrating to use. To find a solution of sufficient quality requires trial and error and a good dose of analysis. Understanding partial solutions to a complex problem is a natural way of finding a solution that is higher in quality and hopefully good enough for the programming language to be usable. The Figure \ref{figCreativeProcess} contains a visualization of how these steps integrate with each other.

\begin{center}
\begin{figure}[h!]
\centering
\begin{tikzpicture}[thick, node/.style = {draw}]
  \node[node] (a) at (0,0) {Analyze implementation};
  \node[node] (b) at (0,3) {Implement and find errors};
  \node[node] (c) at (5,1.5) {External input};

  \draw[->, align=left] (a) to[out=30,in=-30] node[right]{Ideas} (b);
  \draw[->, align=left] (b) to[out=-150,in=150] node[left]{Deeper understanding} (a);
  \draw[->] (c) to[out=-90,in=0] node[right]{Concrete ideas} (a);
  \draw[->] (c) to[out=90,in=0] node[right]{Inspiration} (b);
\end{tikzpicture}
\caption{The creative process of solving complex problems according to the author.}
\label{figCreativeProcess}
\end{figure}
\end{center}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Results
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Results}
% \label{cha:Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementations, Evaluation and Results}
\label{cha:Implementation}
\todo{I removed the Results chapter and keept this instead, since I think all of this is results right?}
The problem of doing wordlength inference is a complex problem. This section contains a series of different implementations that failed in various ways, only the last one implementation ''worked'' and is described in Section \ref{sec:Seven}.

Each section describes the thought process, implementation details of most important, new insights gained from this work and an evaluation of the final state of the implementation for each attempted solution. Explicit commit hashes are given to make it very clear what version is discussed.

Starting the implementation work we knew parts of what we wanted to achieve. We wanted a more sophisticated version of wordlength inference that handled mathematical expressions with less error. The first step was understanding what was currently happening with the wordlength inference and where the problems where placed. The goal of this implementation was to find out just how much we did not know about the problem of a more sophisticated wordlength inferrer.

At the start, the dream was to implement this wordlength inference into the typeinference directly. It was considered somewhat possible since Damas-Hindley-Milner typecheckers have been implemented with sub-typing that could be mapped onto this problem -- one such mapping is Elm with typeinference and extensible records.

In the discussions, some of the less interesting implementation details are deliberately left out. The withheld details aim to make the text clearer and focused.

\section{Context and the Goal of These Experiments}
Most of these implementations were experiments and were not thought to be the solutions most likely to work. The more encompassing and maybe easier solutions have always been adding a separate module for wordlength inference -- in a similar way to how the linear types are checked in Spade at the moment -- or adding a whole new typeinference and typechecker.

The separate wordlength inference module was however not a completely sound solution. Splitting the wordlength inference from the typecheckeer has both pros and cons, since the information from the wordlength inference has to end up in the inferred types from the typeinference the modules need some kind of communication and this communication is obviously easier if the modules are the same module. However the wordlength inference requires information about the expressions and variables used in them, which the typeinference currently throws away. These kinds of errors with the interaction between the typeinference were suspected to be difficult to fix, so a few minor experiments were made into potential easier solutions.

Replacing the typeinference in the current compiler with something more powerful could potentially be the best solution. The current typeinference was regarded by some contributors as magic and was hard to work with and reason about -- in some sense the module was considered technical-debt. This poses a lot of language design questions, questions this thesis hoped to avoid since it might cause features to not be merged and made useful in the mainline compiler. Considering for example that a more powerful typeinference might not be desirable in the language. Since most of the more powerful typechecking-systems are know to have horrid error messages -- the antithesis of Spades goal of friendly error messages -- the change should not be made to quickly out of respect for the project. It would also be a very large undertaking to retrofit the entire compiler to work with this new typeinference and be more work than is available in this thesis and was always considered out of scope. 

\section{The First Implementation -- Naive}
\label{sec:First}
The changes for the first bash at the problem are placed on the git-branch \verb+the-simplest-implementation+ and has git-hash \verb+74c966a0317aa738017d2edf15def4719fe8dc95+.

After throwing a quick glance at the code it became clear that at least lower and upper bounds -- though these changes would not solve the larger problem of inferring the wordlength using Affine Arithmetic only allowing usage of Interval Arithmetic, but adding Affine Arithmetic in the typeinference required a kind of replacement that was unclear how to add at the time. It was also considered reasonable to only support wordlengths that fit in 64 bits -- since the current FPGA design does not handle that kind of data.

After poking some more two different ways of expressing constraints and requirement were found \verb+ConstraintExpr+ and \verb+Requirement+. These very similarly named concepts are very similar. \verb+ConstraintExpr+ is used primarily for compile-time integers and has support for addition, negation, constants and logarithms. \verb+Requirement+ adds information that does not fit neatly into a Damas–Hindley–Milner typesystem, namely: fields a type is expected to have, methods a type is expected to have and if a type can hold a given integer literal. Both \verb+Requirement+s and \verb+ConstraintExpr+s are removed after they are satisfied for a type.

The type checking of binary expressions simply added $1$ to the wordlength for sum-expressions, and summed up the wordlengths for multiplication-expressions. The naive worstcase approach was changed to work using Interval Arithmetic. 

\verb+ConstraintExpr+s looked to be the best suited current construction in the compiler so we decided to hijack it into supporting ranges, to see where the compiler started complaining. \verb+ConstraintExpr+ was changed to support addition, negation, and multiplication while working on a high and a low value not only a single value. The typechecking of binary expressions was also changed to add different constraints.

This change broke a lot of things in the compiler in interesting ways. Array lengths no longer worked since they also used these kinds of \verb+ConstraintExpr+ -- which is very interesting to note. The types of some expressions also failed to infer which complicated the \verb+HIR-lowering+. The reason for the missing types is because there are choices to be made when typechecking some expressions, what wordlength should actually be used when there are multiple candidates. There was also an attempt at mitigating the missing types by introducing an extra solver stage for the \verb+ConstraintExpr+s -- this stage ran after a function was typechecked and tried to pick the smallest size of all constraints. This change was also too large to allow as much inter-op as possible with the mainline spade-compiler.

The implementation outlined here did not work -- it was also never expected to work. It gave very clear hints as to where to dig deeper, where the understanding was sub par and some of the hidden requirements for adding wordlength inference.

\section{The Second Implementation -- Another Naive Stab}
\label{sec:Second}
The second attempt was also very early in the process. Changes are available on the git-branch \verb+the-second-simplest-thing+ and has git-hash \verb+3d92c0e4b28b64104f700c3d299f62e7938cb016+.

Full with unmotivated optimism the \verb+ConstraintExpr+ was extended to support a \verb+BitsToRange+ -- which is the opposite of the \verb+BitsToRepresent+ expression which handles logarithms. The idea was to allow moving between wordlength and a max-value in the type-level expressions. This would hopefully leave the array code working, while allowing the expression code to infer wordlengths at least somewhat freely. This idea and implementation -- was like the implemenetation in Section \ref{sec:First} -- not supposed to solve the entire problem and is not a fit solution for the entire problem since it only solves the sub-problem of very naive interval based wordlength inference.

When reviewing the code for this report an error in the original code was also found -- the function \verb+check_expr_for_replacement+ should refer to \verb+BitsToRange+ on line $1036$ -- the changes presented in this section never compiled.

These new constraints did not suffice, since all of the constraints were equality (\verb+=+) constraints and for this idea to work less-than-or-equal constrains were required (\verb+<=+). Less-than-or-equal constraints might be required for wordlength inference, and it might be why it is fairly hard and unstable to insert these constraints into the current constraint/requirement systems. The problem of adding Affine Arithmetic is still unsolved and would not be resolved using a solution of this form so we need to represent the operations and their values in full. We want to encode the operations and additions into a structure. We also need to carry the information for the intervals of variables somewhere else, since the current typesystem really disagrees with encoding ranges into int-types -- a new construction is required. 

\section{The Third Implementation -- Considering Equations}
\label{sec:Third}
The third attempt focused on adding equations and added a seperate constraint language. These constraints are part of the typeinference.

A new set of constraints was added giving \verb+ConstraintExpr+ and \verb+Requirement+ a new friend called \verb+SizeExpression+. Aside from the obvious technical-debt added with a whopping 3 different kinds of constraints that express almost the same thing -- the solution was deemed too ''ugly''. The constraints needed for \verb+SizeExpression+ were required to be feed forward by the typeinference -- following a completely different approach to the other requirements which were implicitly added to lists. This caused a large amount of changes in the typeinference which was preferably to be avoided.

We also start to suspect that the wordlength inference might \textit{need} to be a seperate module, split up from the typeinference and run after all the types have been inferred. This code did however not live long, and might even be considered mere curiosities.

\section{The Forth Implementation -- A Desperate Attempt}
Almost out of desperation for getting something to work. Just to make sure this whole ordeal wasn't impossible -- these changes were made. These changes are also the most insightful in all of these experiments. The changes are available on the git-branch \verb+the-fourth-attempt-now-with-equivelence+ and has git-hash \verb+ba0fa1baab56a725c31f703204da3b7d5f44380a+.

This implementation sent extra information about the largest number possible to come from the computations with all expressions -- similar to the third implementation in Section \ref{sec:Third} but just better written. \verb+ConstraintExpr+ was also extended with the variant \verb+LargestNumber+ which gives out the largest number representable by the wordlength given in -- the opposite of \verb+BitsToRepresent+ and similar to the extra variant made in the second implementation in Section \ref{sec:Second}. The constraints in the typechecking of binary expressions were changed to either take the largest number from the sub-expression or just the largest number from the wordlength this snippet is presented in Figure \ref{fig:BinaryExpression}. The code creates variables from sub-expressions and creates a hidden type-variable \verb+result_max+, then constraints link these different variables together -- all the constraints are equality (\verb+=+) constraints.

\begin{figure}
\begin{verbatim}
// Let the typechecker infer the maximum ints 
let lhs_max = self.visit_expression(&lhs, ctx, generic_list)?;
let rhs_max = self.visit_expression(&rhs, ctx, generic_list)?;
match *op {
   BinaryOperator::Add => {
       // Create new typevariables
       let (lhs_t, lhs_size) = self.new_split_generic_int(&ctx.symtab);
       let (rhs_t, rhs_size) = self.new_split_generic_int(&ctx.symtab);
       let (result_t, result_size) = self.new_split_generic_int(&ctx.symtab);

       // Create a helper type variable that adds indirection for the unification later
       let result_max = self.new_generic();

       // Take our best guess of a max value
       let lhs_max = match lhs_max {
           Some(max) => ce_var(&max),
           None => ce_largest(ce_var(&lhs_size)),
       };
       let rhs_max = match rhs_max {
           Some(max) => ce_var(&max),
           None => ce_largest(ce_var(&rhs_size)),
       };

      // We know the maximum values add to give a new maximum
      self.add_constraint(
          result_max.clone(),
          lhs_max.clone() + rhs_max.clone(),
          expression.loc(),
      );
      // The maximums implies a size for each of the values
      self.add_constraint(
          result_size.clone(),
          bits_to_store(ce_var(&result_max)),
          expression.loc(),
      );
      self.add_constraint(
          lhs_size.clone(),
          bits_to_store(lhs_max.clone()),
          lhs.loc(),
      );
      self.add_constraint(
          rhs_size.clone(),
          bits_to_store(rhs_max.clone()),
          rhs.loc(),
      );
      // Link the type variables to the AST
      self.unify_expression_generic_error(expression, &result_t, &ctx.symtab)?;
      self.unify_expression_generic_error(&lhs, &lhs_t, &ctx.symtab)?;
      self.unify_expression_generic_error(&rhs, &rhs_t, &ctx.symtab)?;

      // Return our best guess to make the expressions around aware of us
      Ok(Some(result_max))
   }
   ...
}
\end{verbatim}
\caption{The code adding the constraints for addition in the Spade-compiler}
\label{fig:BinaryExpression}
\end{figure}

This approach worked, for some definition of working. This solution behaves poorly and causes some very frustrating error messages. Since the hidden type-variable stored in \verb+result_max+ can cause errors in typevariables that are not able to be referenced in the source code of the program -- this kind of code can cause hard to debug errors. The layer of indirection caused by \verb+result_max+ is required to allow unification of int's of various max number into the same size. The \verb+result_max+ variable is also not unified in the unification process which causes us to create a lot of variables with largely the same information without them being synchronized, this can cause inconsistencies with the typechecker in edgecases. There is also extra information that is sent outside the AST and is not recorded. All of this points strongly to a different approach where a separate module tries to infer the wordlength.

\section{Other Implementations -- Just Curiosities}
\label{sec:Other}
There were two more attempts at an implementation inside the typechecker, they are available on the git-branches \verb+the-sixth-attempt+ and \verb+the-seventh-attempt-almost-as-simple-as-attempt-one+. Both these experiments yielded little of interest since they mostly verified the previous claims, mostly the implementations from Section \ref{sec:First} and Section \ref{sec:Second}. They were also very shallow changes since the more promising change outlined in Section \ref{sec:Seven} was started on instead.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Seventh Implementations -- a Seperate Module}
\label{sec:Seven}
After doing a lot of thinking and reasoning we decided to see how a separate module would look -- in theory this would give maximum flexibility in how the wordlength inference worked. Using a separate module for the wordlength logic also meant changing it in the future would be easier, if more experiments would be conducted. Since the typechecker would not be changed as much old programs could still be compiled and thus compared with and without the more advanced wordlength inferrence. These changes are available on the branch \verb+the-seventh-attempt-almost-as-simple-as-attempt-one+ with git-hash \verb+889afd61a59f04f60730964b8ae7a2703110dd99+, these changes were merged and is available in the PR hosted on \url{https://gitlab.com/spade-lang/spade/-/merge_requests/200}.

The idea for the implementation was to walk the entire AST after typechecking had run and monomorphisation was complete -- each entity was given to the wordlength-inference module called \verb+spade-wordlength-inference+. The algorithm from Figure \ref{fig:WLIAlgo} was then applied. Visit the entire AST, picking out the expressions that are typed to be \verb+int<_>+ and construct algebraic constraints on the form \verb+<var> = <expression>+, this is called an equation. Pick out all typevariables which are \verb+int<_>+ from the previous step, populating a mapping between type-variables and their wordlength. Then loop over all constraints from step 1 and try to evaluate the right-hand side. The evaluation method can be one of three modes: ''Old Mode'', ''Affine Arithmetic'' or ''Interval Arithmetic'', these options could also be enabled with the environment variable \verb+SPADE_INFER_METHOD+ to allow it to work with the swim buildsystem. If evaluation succeeded add the new variable to a new known set of variables and check there are no contradictions. Stop the loop if no more progress is made or we have looped as many times as there are equations. The result is then unified back into the AST for safekeeping. There are of course more details, and the Rust code for wordlenght inference is hopefully quite readable. If you are interested I would recommend reading the source code directly. 

\begin{figure}
\begin{verbatim}
if InferMethod == OLD then return

(Variables, Equations) = extract_variables_and_equations(AST, TypeChecker)
Known = extract_known_types(Variables, AST)

for _ in Equations:
  KnownAtStart = Known
  for (Var, Body) in Equations:
    MaybeValue = case InferMethod of
                  IA -> evaluate_using_ia(Known, Body)
                  AA -> evaluate_using_aa(Known, Body)
    case MaybeValue of
      Just Value -> inject_and_check_for_contradictions(Value, Known)
      Nothing -> pass
  if KnownAtStart == Known:
    break

for Var in Variables:
  unify(TypeChecker, Var, Known[Var])
\end{verbatim}
\caption{The algorithm used in the seventh attempt to determine the wordlength for all expressions in a spade-program.}
\label{fig:WLIAlgo}
\end{figure}

\subsection{Evaluating the Changes}
This implementation of the spade compiler was then run on the example program \verb+spade-memory-display+ using the swim command \verb+SPADE_INFER_METHOD=XX swim pnr+. A simple bash script was made to record the number of memories used in the FPGA when the command was executed and this was stored in a file. The program was compiled 50 times for the three configurations: ''Old Mode'' -- denoted by ''ONE'', ''Affine Arithmetic'' -- denoted by ''AA'', ''Interval Arithmetic'' -- denoted by ''IA'', the number of LUTs used was then feed into a spreadsheet program and produced the table in Figure \ref{fig:SpadeCompilations50Table}.

% \begin{center}
% \begin{figure}
%   \includegraphics[width=\textwidth]{number-of-luts.pdf}
%   \caption{The number of LUTs after PNR for the project spade-memory-display with different versions of wordlength inferrence in bar-chart form.}
%   \label{fig:SpadeCompilations50Bar}
% \end{figure}
% \end{center}

\begin{figure}
\begin{center}
\begin{tabular}{l | c c c}
  & ONE (Old version) & IA (Interval Arithmetic) & AA (Affine Arithmetic) \\
\hline
Average number of LUTs&179.6&176.9 & 177.1 \\
Variance for number of LUTs &13.2&6.0&8.2 \\
Largest number or LUTs&186.0&181.0&183.0 \\
Smallest number of LUTs&171.0&171.0&167.0 \\
\end{tabular}
  \caption{The number of LUTs after PNR for the project spade-memory-display with different versions of wordlength inferrence in table form.}
  \label{fig:SpadeCompilations50Table}
\end{center}
\end{figure}


The Figure \ref{fig:SpadeCompilations50Table} shows a slight decrease in variance for the number of LUTs generated using ''IA (Interval Arithmetic)'' and ''AA (Affine Arithmetic)''. The average stays almost the same for all methods with ''ONE (The Old Version)'' being a bit higher than the others -- but not by a statistical margin.

These changes did shown promise in the usability of the language. Snippets like the one shown in Figure \ref{fig:CodeThatWorksNow} was typechecked a lot better by the spade compiler and in that sense a usability improvement can be seen, note especially the difference in the return type of the function \verb+add_six_times+ where the one typechecked with ''Affine Arithmetic'' always returns 0 and ''Interval Arithmetic'' returns 8 bits -- compared to the ''Old'' approach which returns an integer with 11 bits. % The number of cases where a frivolous truncation was needed has decreased due to these changes. This can lead to clearer and more concise code.

\begin{figure}
\begin{verbatim}
// Old spade wordlength inferrence
fn add_and_subtract(x: int<5>) -> int<11> {
  (x - x) + (x - x) + (x - x)
}

// New spade wordlength inferrence using AA
fn add_and_subtract(x: int<5>) -> int<0> {
  (x - x) + (x - x) + (x - x)
}

// New spade wordlength inferrence using IA
fn add_and_subtract(x: int<5>) -> int<8> {
  (x - x) + (x - x) + (x - x)
}
\end{verbatim}
  \caption{A simple spade function showing the difference in the wordlength inference with the new approaches with a focus on addition and subtraction.}
  \label{fig:CodeThatWorksNow}
\end{figure}

This implementation leaves a gaping hole roughly the size of an elephant. How does a user of the Spade language define these ranges? Since the changes discussed until now have not touched the syntax or the typechecker (except disabling the old wordlenght inferrence), the entire language as a whole is completely blind to the ranged-based wordlength inference. The current approached seamed to work well and it was decided that the best course of action was to extend this implementation.

\section{The Seventh Implementation Extended -- Doubling Down on Ranges}
\label{sec:Seven2}
After the raging success of the implementation in Section \ref{sec:Seven} it was decided that some kind of syntax was needed for these changes to more easily communicate the actual ranges of values. This leads to the changes that are available in the git-branch \verb+wordlength-inference/push-the-changes-further+ with git-commit hash \verb+ee9980c6ec518dbdf0794dbb9193c5b1c9b6945e+, these changes are also a merge request on gitlab which is still up at the time of writing (\url{https://gitlab.com/spade-lang/spade/-/merge_requests/208}). The scoping of these changes was very important. A lot of the details and changes discussed are very opinionated and this thesis tries to not focus too closely on the design of programming languages and more on the actual implementation of the language features, though some background and reason will of course be given. 

There was an earlier attempt similar work previously -- the work had been abandoned since the changes in the typechecker and other compilation steps are non-trivial. The previous work is available in a closed merge request from the official Spade repository \url{https://gitlab.com/spade-lang/spade/-/merge_requests/23}. A problem that was outlined in the merge request was that no changes were made in the typechecker and that the changes were purely syntactical -- which caused some implementation details to be harder. Since there now is a working implementation outlined in Section \ref{sec:Seven} of a range based system the constraints in the typeinference-stage need not be updated. Basing the range based syntax on the newly implemented wordlength inference simplifies a lot of details.

The syntax of the Spade language was first of all changed to parse \verb+int+ types differently, the syntax \verb+int<L..H>+ allowing ranges to be specified. The old syntax \verb+int<W>+ was still supported but was syntactic-sugar for \verb!int<!$-(2^{w+1})$\verb!..!$(2^{w+1})-1$\verb!>! (the $+1$ part comes from all integers being signed in Spade) since this would give partial compatibility with older spade-program. In the previous work a lot more syntaxes were added but they were considered extraneous for updating the typeinference.

The \verb+int+ type also had to change to work with 2 generic arguments -- a lower and a higher bound. These types had to be inserted in the typechecker. Making this change in full would require rewriting almost every single spade-compiler test, and would be a substantial amount of work and time, more time than this thesis has been allotted. The decisions to break with the best practices of software development was taken to make sure the thesis could be finished at all. Some of the tests were fixed where it was deemed simple to do so. Most of these changes were very mechanical though. The wordlength inference code also needed to be changed slightly to accept the new ranges as inputs from the typechecker.

These changes also made the typechecker aware of the range based semantics of integers in the wordlength inferrer -- thus the typechecker could be used to propagate type information about these ranges. This made all library functions which previously only used wordlengths work properly with arbitrary range. Users of the spade language could now also specify ranges on their types themselves.

An extensions was also made to the wordlength inference methods described in Section \ref{sec:Seven}. A naive method that runs both ''Interval Arithmetic'' and ''Affine Arithmetic'' and returns the subset was also introduced.

The very basic implementation in this thesis does not handle memories and registers. Though supporting them should be very minor work, but requires a lot more verification. There are also questions as to the language features of registers and memories should interact with the range-based syntax of integers. The unification rules from the typechecker are also very strict and disallow things like passing a constant to a function that takes an integer argument that is not constant. How these problems are to be solved is according to the thesis authors a matter of taste, though these problems do exists and needs addressing. 

\subsection{Evaluation the Changes}
After a change has been made it needs to be verified and tested. A custom version of the compiler with the changes described in Section \ref{sec:Seven2}. Each program was PNRd 51 times for each of the three configurations ''AA'', ''IA'' and ''AAIA'' -- and a clean of the build environment was conducted between all steps. The changes breaks a lot of compatibility with older Spade-versions which severly limits the number of Spade programs that can be used since a dependency written for a different version of the compiler is not going to compile.

\subsubsection{spade-memory-display}
The small library \verb+spade-memory-display+ was used for evaluation. The changes to the compiler required changes to the library code in order to compile, this made it unclear how to compare directly with older versions of the compiler since small code changes can have large changes in the output. The library code was PNRd 51 times for each of the three configurations ''AA'', ''IA'' and ''AAIA'' with \verb+swim clean+ ran between each build.
 
\begin{figure}
\begin{center}
\begin{verbatim}
ICESTORM_LC: 190/1280 (14.8%)
ICESTORM_PLL: 0/1      (0.0%)
ICESTORM_RAM: 0/16     (0.0%)
SB_GB: 2/8            (25.0%)
SB_IO: 4/112           (3.6%)
SB_WARMBOOT: 0/1       (0.0%)
\end{verbatim}
\end{center}

  \caption{The output from every place and route run given regardless of wordlength inference method for the spade-library spade-memory-display.}
  \label{fig:SMDoutput}
\end{figure}

All builds produced the same metrics and is presented in Figure \ref{fig:SMDoutput}. The metrics shows no difference for any of the 153 place and route compilation of the spade-library. All of the compilations used 190 LUTs. 

\subsubsection{A simple FIR-filter}
FIR-filters are often implemented in hardware since they are of great use for signal processing in general. So a simple FIR-filter program with 100 elements was setup. The code was PNRd 51 times for each of the three configurations ''AA'', ''IA'' and ''AAIA'' with \verb+swim clean+ ran between each build. The FIR-filter program in its entirety is available in Section \ref{source:FIR}.

\begin{figure}
\begin{center}
\begin{verbatim}
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)
\end{verbatim}
\end{center}

  \caption{The output from every place and route run given regardless of wordlength inference method for the simple FIR-filter.}
  \label{fig:FIRoutput}
\end{figure}

The Figure \ref{fig:FIRoutput} shows the output from every PNR run for the FIR-filter program. All the outputs were identical regardless of what wordlength inference method was used. It is also worth noting that for ''IA'' and ''AAIA'' the return type of the \verb+fir+-function can be simplified to \verb+int<0..1000000>+. The return type is required to be \verb+int<-500000..1000000>+ for ''AA'' which goes into the negative even though it cannot possibly happen.

\Todo{Maybe I need to add more programs here or something? I don't know...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{cha:Discussion}

\section{Results}
The Spade compiler was successfully changed to support ranges and more sophisticated wordlength inference. These changes were not able to be completed to the degree that they could replace the mainline compiler -- but most of the harder technical problems have been solved. 

The combination of taking the subset of both ''AA'' and ''IA'' results in the most tight ranges and the smallest wordlengths. That a combination of the techniques is superior is hardly surprising since ''IA'' handles multiplication better for some expressions a combination is bound to work better than any of them in isolation. Of special consideration is how ''IA'' handles $0$ a lot better, since ranges bound by 0 are still bound by $0$ even after multiplication, this is not the case for ''AA''. This begs the question if there is a better method that would give even tighter ranges.

The internal changes to the spade-compiler typeinference caused a sea of troubles to appear. If the Spade project want to integrate these changes fully care needs to be taken when implementing and updating language features like memories. There is also a lot of work needed to port all the tests for the Spade language, all of these are fairly manual changes but might require a lot of time. To fully integrate these changes a series of language design decisions would also need to be taken, for example how constants passed to functions should typecheck. Some changes requires more code changes than others.

There was also no discernible difference between the PNRd programs before and after the wordlength inference change. The table in Figure \ref{fig:SpadeCompilations50Table} -- even though the averages might suggest it. It might be claimed that the difference in variance is an improvement, it might more probably be due to random noise from the compiler since PNR has undeterministic behavior. The figures Figure \ref{fig:SMDoutput} and Figure \ref{fig:FIRoutput} support this claim. Though it is worth noting that something might be different in the compilation process since both of these figures show no noise. This might signal an error in the experiments and that the Spade-implementation with the range-based wordlength inference might have a bug which causes incorrect codegeneration, unfortunately there was no access to hardware to verify this implementation on. This is a major limitation of this study.

\subsection{Limitations in the Spade Compiler}
The implementation offered in this thesis can have quite sporadic error messages. This is due to the typechecker discarding information of where typeinformation came from -- and the typeinference module is left looking at the expressions unless it wishes to implement another typechecker which would have to be in sync with the one already in the language. Changing what the Spade-compiler stores from the typechecking phase would require a fair bit of plumbing but still nothing hugely complicated to implement. For later stages in the compiler it would be preferable to make some changes in the compiler to support:
\begin{itemize}
  \item A list of spans that have been unified to give a type for an expression -- this would make it possible to point to the type signatures from stages that do e.g. constant folding or wordlength inference.
  \item A full list of the constraints and requirements for a type -- currently the compiler tosses these constraints and requirements when they are deemed satisfied, this discards the sources of facts that both the typechecker and wordlength inference module would find helpful.
  \item It would be great if \verb+Requirement+ and \verb+ConstraintExpr+ could be one construction -- it would aid interop and remove some technical debt in the typechecker.
  \item It would also be beneficial if each entity in the Spade program could be compiled as far as possible -- so if typechecking for one function of the program fails the wordlength could still be checked for an entity that is not related.
\end{itemize}

There are also profound profits in the area of usability of the Spade language -- but removing truncation operations from the Spade code can make the code a lot more readable. Since most of the truncations and sign extensions are required by the very simple wordlength inference method present before this thesis this is probably the single largest contribution that has been made. The change in wordlength inference makes the Spade-compiler more flexible and makes it possible to remove a lot of the truncation and extension operations.

\Todo{Research questions?}
\Todo{How can interval arithmetic and affine arithmetic be used to implement wordlength inference?}
\Todo{How does wordlength inference and optimization affect the number of LUTs, DPS-blocks and memories for a circuit?}
\Todo{Can wordlength inference be used to create more reusable code?}

\section{Method}
The method of making multiple changes and seeing what works proved beneficial. It was helpful in introducing new developers into the complex codebase that is the Spade compiler. Most of the work done in those sections -- though discarded -- found either a fault in the reasoning for this thesis or a fault in compiler design. The value of these changes is high, according to the authors.

The method does not focus directly on wordlength inference but focuses more on the software development side, which is in stark contrast with the research questions. This might however show more fault in the usage of research questions than this thesis. The authors do believe the code changes that came from this thesis to be among the best possible for the given time. The fumbling evaluation of alternative implementations give a basis for that argument. It is however possible for such an implementation to have been found by thinking very carefully and very hard -- though theoretically sound ideas often crumble in agony when faced with the sledgehammer that is reality. Especially if those theories are drawn from inexperience.

From software development it is well known that an iterative approach to software development often yields the best results in a fixed amount of time. This programmer lore is true for this thesis.

It is also frustrating to leave the compiler in state where most of the work is almost done. As stated earlier in the thesis -- it is possible that some of the changes needed to be done are in fact harder than they appear. But programming work is well known to be hard to plan for. This thesis does however leave a good base to work further on the compiler. It might also be for the better than these other changes are made by someone on the core Spade-compiler team. Though this thesis has been a collaboration with the Spade-compiler team communication is always lossy, and details are bound to be forgotten. Hopefully these changes are well documented enough to be of use and be able to be well integrated into the language.

\section{The Work in a Wider Context}
More sophisticated wordlength inference in Spade unfortunately has little effect on the planet or the current war in Ukraine that is currently raging (at the time of writing). But this contribution need not be discarded as useless. A slight improvement in efficiency for hardware designs can have cascading effects on how cheap it is to produce custom circuits. The cheaper cost can of course lead to an increased production which is what has already happened to goods such as computers and cars. Hardware description languages and FPGAs is used heavily in the weapons industry and thus code created in this thesis might contribute to more raging wars. That said, war is a double edged sword -- and it might save as well as damn -- but it is sure to always destroy.

Hopefully hardware designers will rejoice over these changes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\label{cha:Conclusion}
Programming languages are complex things -- but they need to be complex to express complex and precise ideas. The wordlenght inference outlined in this thesis shows a good way to extend the Spade hardware description language to more flexibly handle wordlengths. The wordlength inference works well with the current Damas–Hindley–Milner typechecker and will allow language designers of the Spade language to make an active choice in how the Spade language should interact with wordlengths. There was no difference in the resource usages of spade programs before or after the wordlength inference from this paper was introduced. The authors believe that the real benefit from these changes will not come from performance gains -- though better performance might be possible in the future -- but the ''soft'' value of expressiveness and clearer communication. A possibility of clearer intent will make it possible for the Spade-compiler to more clearly understand the programmer and might make other optimizations possible.


\section{How can interval arithmetic and affine arithmetic be used to implement wordlength inference?}
The most optimal way was to combine both interval arithmetic and affine arithmetic. Since both have differnt strengths and weaknesses the combination of the methods costs little in the way of resources but can in some instances give a lot smaller ranges which leads to smaller wordlengths. If this work is to be extended to support unsigned integers this combined method is bound to come in very handy. 

\section{How does wordlength inference and optimization affect the number of LUTs, DPS-blocks and memories for a circuit?}
This thesis clearly shows that this implementation of wordlenght inference has no discernible effect on any resource usage metrics.

\section{Can wordlength inference be used to create more reusable code?}
The final state of the changes from this thesis made the compiler very finicky about wordlengths. The expressiveness in the typesystem has increased which has made it possible to express some types more clearly and making certain function types more expressive, making it possible to do more things inside of functions -- aiding the reusability of code. Though it does not aid the resuability greatly the authors claim this is an improvement no matter how modest it is. That said, future work could easily expand on this.

\section{Future Work}
The implementation used in this thesis opens the door for an even more sophisticated approach where sub-expressions could be evaluated by either ''AA'' or ''IA''. One can consider an evaluation tree where all possible combinations of ''AA'' and ''IA'' are used -- though this tree might be too large to easily evaluate a technique like this would give theoretically optimal wordlengths for expressions. There is of course also the possibility of using a different method than ''AA'' and ''IA'' -- but adding more inference methods might make all the other approaches in the compiler even more potent.

Since the Spade-compiler now understands expressions and the values the expressions can be evaluated to it is trivial to check for expressions that evaluate to a constant. A constant expression is almost always a programmer error, but even when it isn't it should be replaced with the constant for clearer readability. This range analysis can also be used to trim dead code even before the codegeneration, and then warn about unreachable paths.

Since the compiler now reasons about ranges of integer values unsigned integer optimizations should be possible to do in a multitude of places. Maybe even automatically generating code that uses unsigned integers if the code never goes bellow 0. But this can ofcourse be taken a step further, and all expressions could be re-written by the compiler to avoid going bellow 0. We can trivially rewrite the expressions inside the compiler since integer arithmetic is a well behaved ring (unlike floating point numbers). This could lead to improved resource usage in some places. Maybe it could even be possible to shift the representation of numbers inside the compiler.

Some of the typing and inference rules need to be updated to make the language work well with e.g. Constants passed to functions. Looking into these kinds of rules could do a lot of the language, and pairing it with a usability study could be very beneficial.

There are also more advanced wordlenght inference methods. There are extensions to ''AA'' which might prove useful if implemented. Or maybe there are other completely novel methods that can be implemented. Maybe it is possible to track every possible integer value an expression can take, and allowing these holes in the expressions would make it possible to give even more precise guidance and help. Maybe it could be possible to pair the Spade-language with more complex formal verification methods.

Ofcourse all of these are ideas, but might be interesting to look into.

\clearpage

\printbibliography

\appendix
\chapter{Versions and Hashes}
\label{app:VersionsAndGitHashes}
\section{Git Hashes for Development}
All code is available on \href{https://gitlab.com/FredTheDino/spade}{the thesis GitLab}
\begin{verbatim}
branchname: the-simplest-implementation
git-commit: 74c966a0317aa738017d2edf15def4719fe8dc95

branchname: the-second-simplest-thing
git-commit: 3d92c0e4b28b64104f700c3d299f62e7938cb016

branchname: the-third-simplest-with-equations
git-commit: 83d1da48f5010767b9a96aea0a5bca13b7415084

branchname: the-fourth-attempt-now-with-equivelence
git-commit: ba0fa1baab56a725c31f703204da3b7d5f44380a

branchname: the-fifth-attempt-now-without-returns
git-commit: d8e9acbca755dea0c2ee78a014c578064c07d47e

branchname: the-sixth-attempt
git-commit: 11b3d060bc34145fda4918d255b186cb4057f07f

branchname: the-seventh-attempt-almost-as-simple-as-attempt-one
git-commit: 889afd61a59f04f60730964b8ae7a2703110dd99
url: https://gitlab.com/spade-lang/spade/-/merge_requests/200


branchname: wordlength-inference/push-the-changes-further
git-commit: ee9980c6ec518dbdf0794dbb9193c5b1c9b6945e
url: https://gitlab.com/spade-lang/spade/-/merge_requests/208
\end{verbatim}

\section{Versions of programs}

\chapter{Programs for Evaluation}
\label{app:Programs}

\section{A very dumb FIR-filter}

\subsection{PNR Output}
\begin{verbatim}
swim clean; SPADE_INFER_METHOD=IA swim pnr
[INFO] Place and route components:
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)

swim clean; SPADE_INFER_METHOD=AAIA swim pnr
[INFO] Place and route components:
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)

swim clean; SPADE_INFER_METHOD=AA swim pnr
[INFO] Place and route components:
ICESTORM_DSP: 0/8    (0.0%)
ICESTORM_HFOSC: 0/1  (0.0%)
ICESTORM_LC: 2/5280  (0.0%)
ICESTORM_LFOSC: 0/1  (0.0%)
ICESTORM_PLL: 0/1    (0.0%)
ICESTORM_RAM: 0/30   (0.0%)
ICESTORM_SPRAM: 0/4  (0.0%)
IO_I3C: 0/2          (0.0%)
SB_GB: 0/8           (0.0%)
SB_I2C: 0/2          (0.0%)
SB_IO: 13/96        (13.5%)
SB_LEDDA_IP: 0/1     (0.0%)
SB_RGBA_DRV: 0/1     (0.0%)
SB_SPI: 0/2          (0.0%)
SB_WARMBOOT: 0/1     (0.0%)
\end{verbatim}

\subsection{Source Code}
\label{source:FIR}
\todo{Not sure I want to keep this in the report -- I don't think it adds much value, and a simple link to the gitlab page might suffice.}
\begin{verbatim}
fn xx<#C>(x: int<C..C>) -> int<0..100> {
  sext(x)
}

fn fir(fs: (int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>)
      ,xs: (int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,
            int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>,int<0..100>)
      ) -> int<-500000..1000000> {
  fs#0 * xs#0 + fs#1 * xs#1 + fs#2 * xs#2 + fs#3 * xs#3 + fs#4 * xs#4 + fs#5 * xs#5 + fs#6 * xs#6 + fs#7 * xs#7 + fs#8 * xs#8 + fs#9 * xs#9 + 
  fs#10 * xs#10 + fs#11 * xs#11 + fs#12 * xs#12 + fs#13 * xs#13 + fs#14 * xs#14 + fs#15 * xs#15 + fs#16 * xs#16 + fs#17 * xs#17 + fs#18 * xs#18 + fs#19 * xs#19 + fs#20 * xs#20 + 
  fs#21 * xs#21 + fs#22 * xs#22 + fs#23 * xs#23 + fs#24 * xs#24 + fs#25 * xs#25 + fs#26 * xs#26 + fs#27 * xs#27 + fs#28 * xs#28 + fs#29 * xs#29 + fs#30 * xs#30 +
  fs#31 * xs#31 + fs#32 * xs#32 + fs#33 * xs#33 + fs#34 * xs#34 + fs#35 * xs#35 + fs#36 * xs#36 + fs#37 * xs#37 + fs#38 * xs#38 + fs#39 * xs#39 + fs#40 * xs#40 +
  fs#41 * xs#41 + fs#42 * xs#42 + fs#43 * xs#43 + fs#44 * xs#44 + fs#45 * xs#45 + fs#46 * xs#46 + fs#47 * xs#47 + fs#48 * xs#48 + fs#49 * xs#49 + fs#50 * xs#50 +
  fs#51 * xs#51 + fs#52 * xs#52 + fs#53 * xs#53 + fs#54 * xs#54 + fs#55 * xs#55 + fs#56 * xs#56 + fs#57 * xs#57 + fs#58 * xs#58 + fs#59 * xs#59 + fs#60 * xs#60 +
  fs#61 * xs#61 + fs#62 * xs#62 + fs#63 * xs#63 + fs#64 * xs#64 + fs#65 * xs#65 + fs#66 * xs#66 + fs#67 * xs#67 + fs#68 * xs#68 + fs#69 * xs#69 + fs#70 * xs#70 +
  fs#71 * xs#71 + fs#72 * xs#72 + fs#73 * xs#73 + fs#74 * xs#74 + fs#75 * xs#75 + fs#76 * xs#76 + fs#77 * xs#77 + fs#78 * xs#78 + fs#79 * xs#79 + fs#80 * xs#80 +
  fs#81 * xs#81 + fs#82 * xs#82 + fs#83 * xs#83 + fs#84 * xs#84 + fs#85 * xs#85 + fs#86 * xs#86 + fs#87 * xs#87 + fs#88 * xs#88 + fs#89 * xs#89 + fs#90 * xs#90 +
  fs#91 * xs#91 + fs#92 * xs#92 + fs#93 * xs#93 + fs#94 * xs#94 + fs#95 * xs#95 + fs#96 * xs#96 + fs#97 * xs#97 + fs#98 * xs#98 + fs#99 * xs#99
}

entity main(clk: clock, rst: bool) -> int<-500000..1000000> {
  let p : int<-500000..1000000> = fir(
    (xx(0),xx(1),xx(2),xx(3),xx(4),xx(5),xx(6),xx(7),xx(8),xx(9)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(20),xx(21),xx(22),xx(23),xx(24),xx(25),xx(26),xx(27),xx(28),xx(29)
    ,xx(30),xx(31),xx(32),xx(33),xx(34),xx(35),xx(36),xx(37),xx(38),xx(39)
    ,xx(40),xx(41),xx(42),xx(43),xx(44),xx(45),xx(46),xx(47),xx(48),xx(49)
    ,xx(50),xx(51),xx(52),xx(53),xx(54),xx(55),xx(56),xx(57),xx(58),xx(59)
    ,xx(60),xx(61),xx(62),xx(63),xx(64),xx(65),xx(66),xx(67),xx(68),xx(69)
    ,xx(70),xx(71),xx(72),xx(73),xx(74),xx(75),xx(76),xx(77),xx(78),xx(79)
    ,xx(80),xx(81),xx(82),xx(83),xx(84),xx(85),xx(86),xx(87),xx(88),xx(89)
    ,xx(90),xx(91),xx(92),xx(93),xx(94),xx(95),xx(96),xx(97),xx(98),xx(99)
    ),
    (xx(99),xx(98),xx(97),xx(96),xx(95),xx(94),xx(93),xx(92),xx(91),xx(90)
    ,xx(89),xx(88),xx(87),xx(86),xx(85),xx(84),xx(83),xx(82),xx(81),xx(80)
    ,xx(79),xx(78),xx(77),xx(76),xx(75),xx(74),xx(73),xx(72),xx(71),xx(70)
    ,xx(69),xx(68),xx(67),xx(66),xx(65),xx(64),xx(63),xx(62),xx(61),xx(60)
    ,xx(59),xx(58),xx(57),xx(56),xx(55),xx(54),xx(53),xx(52),xx(51),xx(50)
    ,xx(49),xx(48),xx(47),xx(46),xx(45),xx(44),xx(43),xx(42),xx(41),xx(40)
    ,xx(39),xx(38),xx(37),xx(36),xx(35),xx(34),xx(33),xx(32),xx(31),xx(30)
    ,xx(29),xx(28),xx(27),xx(26),xx(25),xx(24),xx(23),xx(22),xx(21),xx(20)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(9),xx(8),xx(7),xx(6),xx(5),xx(4),xx(3),xx(2),xx(1),xx(0)
    )
  );

  fir(
    (xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ,xx(10),xx(11),xx(12),xx(13),xx(14),xx(15),xx(16),xx(17),xx(18),xx(19)
    ),
    (xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(19),xx(18),xx(17),xx(16),xx(15),xx(14),xx(13),xx(12),xx(11),xx(10)
    ,xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1),xx(1)
    )
  )
}
\end{verbatim}

\section{Program B}

\todos

\end{document}
